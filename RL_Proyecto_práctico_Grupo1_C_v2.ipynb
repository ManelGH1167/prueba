{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "* Alumno 1: Benali, Abdelilah \n",
    "* Alumno 2: Cuesta Cifuentes, Jair \n",
    "* Alumno 3: González Huete, Manel\n",
    "* Alumno 4: Manzanas Mogrovejo, Francisco\n",
    "* Alumno 5: Pascual, Guadalupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S_YDFwZ-JscI"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "I6n7MIefJ21i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['.anaconda', '.cache', '.conda', '.condarc', '.config', '.continuum', '.dia', '.git', '.gitconfig', '.gitignore', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.Ld9VirtualBox', '.lesshst', '.matplotlib', '.virtual_documents', '.vscode', '01MAIR_ACT_Video.ipynb', '01MIAR_00_Intro.ipynb', '01MIAR_01_Python101.ipynb', '01MIAR_02_Python101_DataTypes.ipynb', '01MIAR_03_Python101_Control.ipynb', '01MIAR_04_Python101_Functions.ipynb', '01MIAR_05_Python101_Files.ipynb', '01MIAR_06_Python101_OOP.ipynb', '01MIAR_07_Python101_Advanced.ipynb', '01MIAR_08_NumPy.ipynb', '01MIAR_09_Pandas.ipynb', '01MIAR_10_+Pandas.ipynb', '01MIAR_11_Visualization.ipynb', '01MIAR_12_Data_Processing.ipynb', '01MIAR_13_Generators.ipynb', '01MIAR_14_Natural_Language.ipynb', '01MIAR_15_OCR.ipynb', '01MIAR_16_Image_Analysis.ipynb', '01MIAR_ACT_Actividad_Final.ipynb', '01MIAR_ACT_Final.ipynb', '01MIAR_ACT_Group.ipynb', '01MIAR_ACT_Group_Solved.ipynb', '01MIAR_ACT_WhitePapers_Canarias.ipynb', '01MIAR_ACT_WhitePapers_Canarias_extendido.ipynb', '01MIAR_Exam_01_B.ipynb', '01MIAR_Exam_Demo.ipynb', '08MIAR_a3c.ipynb', '08MIAR_intro_gym.ipynb', '100_Numpy_exercises.ipynb', '100_Numpy_exercises_with_hints.md', '100_Numpy_exercises_with_solutions.md', 'a3c_full.py', 'Actividad_C1_Manel_Gonzalez_Huete (1).ipynb', 'Actividad_C1_Manel_Gonzalez_Huete.ipynb', 'AG3_Algoritmos(Colonia_de_Hormigas).ipynb', 'AI-blog', 'Algoritmos_AG3 - copia.ipynb', 'Algoritmos_AG3.ipynb', 'AppData', 'breakout_a3c.pth', 'breakout_a3c_best.pth', 'checkpoints', 'Configuración local', 'Contacts', 'Cookies', 'dataset_exam.npy', 'Datos de programa', 'Desktop', 'diagnosticos', 'Documents', 'Downloads', 'dwhelper', 'Ejercicios_evaluables_GrupoC_2.ipynb', 'Entorno de red', 'evaluacion_funciones_5.py', 'Examen_C1_Manel_Gonzalez_Huete.ipynb', 'Favorites', 'Impresoras', 'install.bat', 'JoplinBackup', 'joplin_crash_dump_20240426T174304.json', 'Links', 'Menú Inicio', 'MIAR_23OCT_Exam01-1.ipynb', 'Mis documentos', 'models', 'Music', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TM.blf', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{5b9746f1-f947-11ef-a78f-df08df9682fe}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'OneDrive', 'oo.ipynb', 'Pictures', 'Plantillas', 'ppppp.ipynb', 'Programa15.Clasificacion.LOGR.ipynb', 'Programa16.Clasificacion.CART.ipynb', 'Programa17.Clasificacion.SVM.ipynb', 'README.md', 'Reciente', 'requirements.txt', 'requirements_v2.txt', 'RL_Proyecto_práctico_Grupo1_C (1).ipynb', 'RL_Proyecto_práctico_Grupo1_C.ipynb', 'RL_Proyecto_práctico_Grupo1_C.py', 'RL_Proyecto_práctico_Grupo1_C_v2.ipynb', 'rule_extractor_robotrader.ipynb', 'Saved Games', 'scikit_learn_data', 'Searches', 'Seminario_Algoritmos_Manel Gonzalez Huete.ipynb', 'SendTo', 'swiss42.tsp', 'swiss42.tsp.gz', 'test.py', 'throttle_normal_mode.xml', 'throttle_silent_mode.xml', 'to_install', 'Untitled.ipynb', 'Untitled1 (1).ipynb', 'Untitled1.ipynb', 'Untitled221.ipynb', 'Videos', '_RL_Proyecto_práctico_Grupo1_C_v2.ipynb', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UbVRjvHCJ8UF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IN_LOCAL = True\n",
    "\n",
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.8\n",
    "if not IN_LOCAL:\n",
    "  %pip install numpy==1.23.5\n",
    "  %pip install gym==0.17\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0\n",
    "  %pip install matplotlib==3.4.3\n",
    "  %pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j3eRhgI-Gb2a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc       # Para garbage collection\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import re       # Para expresiones regulares en carga de checkpoints\n",
    "import gym      # Para el entorno de Atari\n",
    "import cv2     # Para preprocesamiento de imágenes si se usa AtariProcessor\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.layers import Lambda, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from collections import deque\n",
    "from tqdm import trange     # Necesaria para la barra de progreso en simple_train\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Necesario para la grabación de video\n",
    "try:\n",
    "    import gym.wrappers\n",
    "except ImportError:\n",
    "    print(\"WARNING: gym.wrappers no está disponible. La grabación de video no funcionará.\")\n",
    "    gym.wrappers = None # Asegurar que no dé error si no se encuentra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar TensorFlow para CPU (14 cores)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "# Ajuste recomendado (seguro y eficiente)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(8)   # dentro de cada operación\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)   # entre operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Crear el entorno\n",
    "Nuestro entorno es el juego Space Invaders, de Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jwOE6I_KGb2a"
   },
   "outputs": [],
   "source": [
    "# Create our environment\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name, obs_type='rgb')\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
      "El número de acciones posibles es :  6\n",
      "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "\n",
      "OHE de las acciones posibles: \n",
      " [[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"El tamaño de nuestro 'frame' es: \", env.observation_space)\n",
    "print(\"El número de acciones posibles es : \", nb_actions)\n",
    "print(\"Las acciones posibles son : \",env.env.get_action_meanings())\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
    "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIPERPARÁMETROS DEL MODELO\n",
    "INPUT_SHAPE = (84, 84)\n",
    "state_size = [84, 84, 3]          # Nuestra entrada es una pila de 4 fotogramas, por lo tanto 110x84x4 (ancho, alto, canales)\n",
    "action_size = env.action_space.n  # 6 acciones posibles\n",
    "learning_rate =  0.00025          # Alfa (también conocido como tasa de aprendizaje)\n",
    "\n",
    "### HIPERPARÁMETROS DE ENTRENAMIENTO\n",
    "# total_episodios = 10    #TEST        # Episodios totales para el entrenamiento\n",
    "# max_steps = 10000       #TEST        # Máximo de pasos posibles por episodio\n",
    "total_episodios = 100          # Episodios totales para el entrenamiento\n",
    "max_steps = 3000               # Máximo de pasos posibles por episodio\n",
    "batch_size = 32                # Tamaño del lote (batch)\n",
    "\n",
    "# Parámetros de exploración para la estrategia epsilon-greedy\n",
    "epsilon_start = 1.0            # Probabilidad de exploración al inicio\n",
    "epsilon_stop = 0.01            # Probabilidad mínima de exploración\n",
    "decay_rate = 0.00001           # Tasa de decaimiento exponencial para la probabilidad de exploración\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "# Hiperparámetros del aprendizaje Q\n",
    "gamma = 0.95                   # Tasa de descuento\n",
    "tau = 0.001\n",
    "checkpoint_path=\"checkpoints\"\n",
    "\n",
    "### HIPERPARÁMETROS DE MEMORIA\n",
    "pretrain_length = batch_size   # Número de experiencias almacenadas en la memoria al inicializar por primera vez\n",
    "memory_size = 5000             # Número de experiencias que la memoria puede guardar\n",
    "\n",
    "### HIPERPARÁMETROS DE PREPROCESAMIENTO\n",
    "WINDOW_LENGTH = 3              # Número de fotogramas apilados\n",
    "\n",
    "### CAMBIA ESTO A FALSE SI SOLO QUIERES VER AL AGENTE ENTRENADO\n",
    "training = False\n",
    "\n",
    "## CAMBIA ESTO A TRUE SI QUIERES RENDERIZAR EL ENTORNO\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase \"processor\" para Atari\n",
    "\n",
    "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tamaño de la imagen (matriz de 210 x 160 píxeles) y la convertimos En una matriz bidimensional de 80 x 80 píxeles). También convertimos las imágenes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la información innecesaria y reduciendo la carga de la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    \"\"\"\n",
    "    Procesador para preprocesar observaciones del entorno Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Hereda de rl.core.Processor y proporciona métodos para convertir observaciones RGB en\n",
    "    imágenes en escala de grises, redimensionarlas y normalizarlas, así como para limitar\n",
    "    las recompensas.\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        process_observation(observation): Convierte una observación RGB a escala de grises\n",
    "                                         y la redimensiona.\n",
    "        process_state_batch(batch): Normaliza un lote de estados dividiendo por 255.\n",
    "        process_reward(reward): Limita las recompensas a un rango [-1, 1].\n",
    "    \"\"\"    \n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocesa una observación convirtiéndola a escala de grises y redimensionándola.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            observation (np.ndarray): Observación cruda del entorno con forma (height, width, channels).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Imagen en escala de grises redimensionada a INPUT_SHAPE (84, 84) en formato uint8.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: Si la observación no tiene 3 dimensiones o la forma procesada no coincide con INPUT_SHAPE.\n",
    "        \"\"\"\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.resize(img, INPUT_SHAPE, interpolation=cv2.INTER_NEAREST)\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        del img\n",
    "        return processed_observation.astype('uint8')\n",
    "    \n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Normaliza un lote de estados dividiendo los valores por 255.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
    "        \"\"\"\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        \"\"\"\n",
    "        Limita las recompensas a un rango [-1, 1].\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            reward (float): Recompensa cruda del entorno.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            float: Recompensa limitada en el rango [-1, 1].\n",
    "        \"\"\"\n",
    "        return np.clip(reward, -1., 1.)\n",
    "    \n",
    "\n",
    "    def process_step(self, reward, terminal, metrics):\n",
    "        return self.process_reward(reward), terminal, metrics    \n",
    "    \n",
    "    def process_action(self, action):\n",
    "        return action    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisar el entorno de juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(22): \n",
    "  if i > 20:\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "\n",
    "  observation, reward, done, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las entradas preprocesadas en escala de grises y comparar originales y preprocesados.\n",
    "processor = AtariProcessor()\n",
    "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
    "# Seleccionamos el primer frame y lo normalizamos\n",
    "frame = processor.process_state_batch(obs_preprocessed)\n",
    "# Visualizar en escala de grises\n",
    "plt.imshow(frame, cmap='gray')\n",
    "plt.show()\n",
    "print(observation.shape)\n",
    "print(obs_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase FrameStack para apilar frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStack:\n",
    "    \"\"\"\n",
    "    Clase para gestionar una pila de fotogramas consecutivos del entorno, utilizada para capturar\n",
    "    el contexto temporal en juegos de Atari (e.g., SpaceInvaders-v0).\n",
    "\n",
    "    Mantiene una cola (deque) de fotogramas preprocesados con un tamaño máximo definido por\n",
    "    max_length, apilándolos para formar un estado con información de movimiento.\n",
    "\n",
    "    Atributos:\n",
    "    ----------\n",
    "        frames (deque): Cola de fotogramas preprocesados con longitud máxima max_length.\n",
    "        max_length (int): Número máximo de fotogramas a apilar (e.g., WINDOW_LENGTH).\n",
    "\n",
    "    MÉTODOS:\n",
    "    --------\n",
    "        append(frame): Añade un nuevo fotograma a la pila, eliminando el más antiguo si es necesario.\n",
    "        get_stacked_state(): Devuelve el estado apilado como un array NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_frames=4):\n",
    "        \"\"\"\n",
    "        Inicializa la pila de fotogramas.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            max_length (int): Número máximo de fotogramas a mantener en la pila.\n",
    "        \"\"\"\n",
    "        self.num_frames = num_frames\n",
    "        self.frames = deque(maxlen=num_frames)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.frames.clear()\n",
    "    \n",
    "    def add_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Añade un fotograma preprocesado a la pila.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "            frame (np.ndarray): Fotograma preprocesado (e.g., imagen en escala de grises de 84x84).\n",
    "        \"\"\"\n",
    "        # Si es el primer frame, llenamos el deque\n",
    "        if len(self.frames) == 0:\n",
    "            for _ in range(self.num_frames):\n",
    "                self.frames.append(frame)\n",
    "        else:\n",
    "            self.frames.append(frame)\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Devuelve el estado apilado como un array NumPy con los fotogramas actuales.\n",
    "\n",
    "        Si la pila no está llena, repite el último fotograma hasta completar max_length.\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "            np.ndarray: Array de forma (84, 84, max_length) con los fotogramas apilados.\n",
    "        \"\"\"\n",
    "        # Convertir a array con shape (84, 84, 4)\n",
    "        return np.stack(self.frames, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory2:\n",
    "    \"\"\"\n",
    "    Clase para implementar un búfer de memoria de repetición.\n",
    "    Almacena transiciones (estado, acción, recompensa, siguiente estado, done) y permite muestreo aleatorio.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        # print(f\"[DEBUG - ReplayMemory __init__] Memoria interna inicializada como: {type(self.memory)}\")       \n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Guarda una transición.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sampleS(self, batch_size):\n",
    "        \"\"\"Muestra un lote de transiciones aleatoriamente.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # ... (sampling logic)\n",
    "        print(\"Forma de states en memory.sample:\", states.shape)  # Depuración\n",
    "        return states, actions, rewards, next_states, dones    \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Devuelve el tamaño actual de la memoria.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def can_sample(self, batch_size):\n",
    "        \"\"\"Verifica si hay suficientes transiciones para muestrear.\"\"\"\n",
    "        return len(self.memory) >= batch_size    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, state_shape):\n",
    "        self.capacity = capacity\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "        self.states = np.zeros((capacity, *state_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.float32)\n",
    "        self.dones = np.zeros(capacity, dtype=np.float32)\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.states[self.index] = state\n",
    "        self.actions[self.index] = action\n",
    "        self.rewards[self.index] = reward\n",
    "        self.next_states[self.index] = next_state\n",
    "        self.dones[self.index] = done\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        return (\n",
    "            self.states[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices],\n",
    "            self.next_states[indices],\n",
    "            self.dones[indices]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "### 1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de las redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proceso no usaremos DQNAgent de keras-rl2, en su lugar desarrolla clases personalizadas (DQNetwork, DDQNetwork, DDQNetworkWithReplay):\n",
    "\n",
    "* Propósito Educativo: Para aprender y controlar cada aspecto de DQN/DDQN, analizando la lógica desde cero.\n",
    "* Personalización: Para implementar variantes como DDQN y memoria de repetición personalizada, que no son directamente soportadas por DQNAgent.\n",
    "* Optimización en CPU: Para reducir el uso de memoria y optimizar el rendimiento en un entorno sin GPU.\n",
    "* Flexibilidad Experimental: Para facilitar la comparación entre DQN, DDQN, y DDQN con replay, y permitir ajustes.\n",
    "* Evitar Limitaciones de keras-rl2: Para superar restricciones, como la falta de soporte nativo para DDQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "O4GKrfWSGb2b"
   },
   "outputs": [],
   "source": [
    "class DQNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Red neuronal Deep Q-Network (DQN) para aproximar la función Q en aprendizaje por refuerzo.\n",
    "\n",
    "    Esta clase implementa una red convolucional que recibe un estado (conjunto de frames)\n",
    "    y produce los valores Q para cada acción posible. Usa capas convolucionales seguidas\n",
    "    de capas totalmente conectadas, con activación ELU.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    state_size : tupla/lista - Dimensiones del estado de entrada (por ejemplo, [84, 84, 4]).\n",
    "    action_size : int        - Número de acciones posibles en el entorno.\n",
    "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
    "    name : str, opcional     - Nombre del scope de TensorFlow para distinguir múltiples redes.\n",
    "    \"\"\"   \n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        super(DQNetwork, self).__init__(name=name)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Crear la red\n",
    "        # Renombrado de self.model a self.main_network para consistencia\n",
    "        self.main_network = self._crear_red('dqn_main') \n",
    "\n",
    "        # Construir el modelo\n",
    "        input_shape = (None,) + tuple(state_size)\n",
    "        self.main_network.build(input_shape)\n",
    "        \n",
    "        # Definir el optimizador\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        # Definir la función de pérdida (error cuadrático medio)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "    def _crear_red(self, network_name): \n",
    "        \"\"\"\n",
    "        Construye una red individual con la arquitectura DQN.\n",
    "               \n",
    "        Retorna:\n",
    "        --------\n",
    "            tf.keras.Sequential - Red neuronal construida.\n",
    "        \"\"\"\n",
    "        return tf.keras.Sequential([\n",
    "            # Primera capa convolucional con activación ELU            \n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv1'),\n",
    "            # Segunda capa convolucional con activación ELU            \n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv2'),\n",
    "            # Tercera capa convolucional con activación ELU            \n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv3'),\n",
    "            # Aplanar la salida de la última convolucional            \n",
    "            tf.keras.layers.Flatten(name='dqn_flatten'),\n",
    "            # Capa completamente conectada con activación ELU            \n",
    "            tf.keras.layers.Dense(units=256, activation='relu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_fc'),\n",
    "            # Capa de salida que devuelve valores Q para cada acción            \n",
    "            tf.keras.layers.Dense(units=self.action_size, activation=None,\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_output')\n",
    "        ], name='dqn_network')\n",
    "\n",
    "    # Usa esto para evitar recompilaciones:\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None, 84, 84, 3), dtype=tf.float32)\n",
    "    ])    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Ejecuta la red neuronal para un batch de estados y define la lógica para hacer la propagación hacia adelante \n",
    "        (forward pass).  Se llama automáticamente en una clase que hereda de tf.keras.Model o tf.keras.layers.Layer.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor  - Tensor con los estados de entrada, shape = (batch_size, *state_size)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q para cada acción, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs)    \n",
    "\n",
    "    # @tf.function    \n",
    "    def train_step(self, states, actions, target_q):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento: calcula la pérdida y aplica gradientes.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        states : tf.Tensor -    Batch de estados de entrada, shape = (batch_size, *state_size)\n",
    "        actions : tf.Tensor -   Acciones tomadas, codificadas one-hot, shape = (batch_size, action_size)\n",
    "        target_q : tf.Tensor -  Valores objetivo Q, shape = (batch_size,)\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor -      Valor de la pérdida calculada en este paso.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.call(states)  # Salida con valores Q para todas las acciones\n",
    "            # Obtenemos Q para las acciones tomadas multiplicando por la máscara one-hot y sumando\n",
    "            q_action = tf.reduce_sum(q_values * actions, axis=1)\n",
    "            # Si target_q es (batch_size, num_actions), entonces:\n",
    "            target_q_action = tf.reduce_sum(tf.expand_dims(target_q, axis=1) * actions, axis=1)            \n",
    "            # Calculamos la pérdida MSE entre Q predicho y target_Q\n",
    "            loss = self.loss_fn(target_q_action, q_action)\n",
    "\n",
    "        # Calculamos los gradientes y actualizamos los pesos\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DDQNetwork\n",
    "class DDQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Crear red principal\n",
    "        self.main_network = self._build_network()\n",
    "        # Crear red objetivo\n",
    "        self.target_network = self._build_network()\n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "\n",
    "        # Optimizador\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "    def _build_network(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, (8, 8), strides=4, activation='relu', input_shape=(84, 84, 3)),\n",
    "            tf.keras.layers.Conv2D(32, (4, 4), strides=2, activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    # Usa esto para evitar recompilaciones:\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None, 84, 84, 3), dtype=tf.float32)\n",
    "    ])     \n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante usando la red principal.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        training : bool    - Si está en modo entrenamiento (default: True)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red principal, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs, training=training)        \n",
    "    \n",
    "    def train_step_optimized(self, states, actions_one_hot, rewards, next_states, dones, gamma):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Depuración: Verificar formas de los tensores\n",
    "            ##########print(\"Forma de states antes de expand_dims:\", states.shape)\n",
    "            ##########print(\"Forma de next_states antes de expand_dims:\", next_states.shape)\n",
    "\n",
    "            # Añadir dimensión de batch si falta\n",
    "            states = tf.expand_dims(states, axis=0) if len(states.shape) == 3 else states\n",
    "            next_states = tf.expand_dims(next_states, axis=0) if len(next_states.shape) == 3 else next_states\n",
    "\n",
    "            # Depuración: Verificar formas después de expand_dims\n",
    "            ##########print(\"Forma de states después de expand_dims:\", states.shape)\n",
    "            ##########print(\"Forma de next_states después de expand_dims:\", next_states.shape)\n",
    "\n",
    "            # Verificar grafos (mantener para consistencia)\n",
    "            ##########print(\"Grafo de states:\", states.graph)\n",
    "            ##########print(\"Grafo de main_network kernel:\", self.main_network.layers[0].kernel.graph)\n",
    "\n",
    "            # Obtener Q-valores de la red principal\n",
    "            q_values = self.main_network(states, training=True)\n",
    "            q_values = tf.reduce_sum(q_values * actions_one_hot, axis=1, keepdims=True)\n",
    "\n",
    "            # Obtener Q-valores de la red objetivo para los siguientes estados\n",
    "            next_q_values_main = self.main_network(next_states, training=False)\n",
    "            next_q_values_target = self.target_network(next_states, training=False)\n",
    "\n",
    "            # Seleccionar la acción con el mayor Q-valor de la red principal\n",
    "            max_actions = tf.argmax(next_q_values_main, axis=1, output_type=tf.int32)\n",
    "            batch_indices = tf.range(tf.shape(next_q_values_target)[0], dtype=tf.int32)\n",
    "            max_actions_indices = tf.stack([batch_indices, max_actions], axis=1)\n",
    "\n",
    "            # Obtener los Q-valores objetivo correspondientes a las acciones seleccionadas\n",
    "            next_q_values = tf.gather_nd(next_q_values_target, max_actions_indices)\n",
    "            target_q_values = rewards + (1.0 - dones) * gamma * next_q_values\n",
    "\n",
    "            # Calcular la pérdida\n",
    "            loss = tf.reduce_mean(tf.square(target_q_values - q_values))\n",
    "\n",
    "        # Calcular gradientes y aplicar optimización\n",
    "        gradients = tape.gradient(loss, self.main_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNetwork2(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Double Deep Q-Network (DDQN), para reducir la sobreestimación en DQN.\n",
    "    \n",
    "    DDQN usa dos redes: una principal (main) y una objetivo (target).\n",
    "    La red principal selecciona las acciones, mientras que la red objetivo\n",
    "    evalúa los valores Q, reduciendo así el sesgo de sobreestimación.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    state_size : tupla/lista - Dimensiones del estado de entrada (por ejemplo, [84, 84, 4]).\n",
    "    action_size : int        - Número de acciones posibles en el entorno.\n",
    "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
    "    tau : float, opcional    - Factor de actualización suave para la red objetivo (default: 0.001).\n",
    "    name : str, opcional     - Nombre del scope de TensorFlow.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, learning_rate, tau=0.001, gamma=0.99, name='DDQNetwork'):\n",
    "        super(DDQNetwork, self).__init__(name=name)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = tf.constant(gamma, dtype=tf.float32)  # Convertir gamma a tensor\n",
    "      \n",
    "        # Red principal (main network)\n",
    "        self.main_network = self._crear_red('ddqn_main')\n",
    "        # Red objetivo (target network)\n",
    "        self.target_network = self._crear_red('ddqn_target')\n",
    "        \n",
    "        # Construir ambas redes con la forma de entrada correcta\n",
    "        input_shape = (None,) + tuple(state_size)  # (None, 84, 84, 4)\n",
    "        self.main_network.build(input_shape)\n",
    "        self.target_network.build(input_shape)        \n",
    "        self.target_network.set_weights(self.main_network.get_weights())\n",
    "        \n",
    "        # Definir optimizador y función de pérdida\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        # Inicializar la red objetivo con los mismos pesos que la principal\n",
    "        self.update_target_network(tau=1.0)\n",
    "\n",
    "        # CRITICAL: Pre-alocar tensores para evitar creación constante\n",
    "        self._preallocate_tensors()                \n",
    "        # PRE-COMPILAR funciones críticas\n",
    "        self._compile_functions()    \n",
    "    \n",
    "    # @tf.function\n",
    "    def compute_loss(self, states, actions, rewards, next_states, dones, gamma):\n",
    "        q_values = self.main_network(states)\n",
    "        next_q_values = self.target_network(next_states)\n",
    "        max_next_q = tf.reduce_max(next_q_values, axis=1)\n",
    "        target_q = rewards + (1.0 - tf.cast(dones, tf.float32)) * gamma * max_next_q\n",
    "        selected_q = tf.reduce_sum(q_values * actions, axis=1)\n",
    "        loss = tf.reduce_mean(tf.square(target_q - selected_q))\n",
    "        return loss\n",
    "    \n",
    "    def _compile_functions(self):\n",
    "        \"\"\"Pre-compilar funciones críticas en graph mode.\"\"\"\n",
    "        # Forzar compilación de funciones críticas usando tensores prealocados\n",
    "        self.main_network(self.dummy_state)\n",
    "        self.target_network(self.dummy_state)\n",
    "        self.compute_loss(self.dummy_batch, self.dummy_actions, self.dummy_rewards, \n",
    "                          self.dummy_next_batch, self.dummy_dones, self.dummy_gamma)\n",
    "        print(\"[INFO] Funciones críticas pre-compiladas\")        \n",
    "\n",
    "    def _preallocate_tensors(self):\n",
    "        \"\"\"Pre-alocar tensores para evitar memory leaks.\"\"\"\n",
    "        # Tensores dummy para batch size 1\n",
    "        self.dummy_state = tf.zeros((1,) + tuple(self.state_size), dtype=tf.float32)\n",
    "        self.dummy_batch = tf.zeros((32,) + tuple(self.state_size), dtype=tf.float32)\n",
    "        self.dummy_next_batch = tf.zeros((32,) + tuple(self.state_size), dtype=tf.float32)  # Nuevo tensor para next_states\n",
    "        self.dummy_actions = tf.zeros((32, self.action_size), dtype=tf.float32)\n",
    "        self.dummy_rewards = tf.zeros((32,), dtype=tf.float32)\n",
    "        self.dummy_dones = tf.zeros((32,), dtype=tf.bool)\n",
    "        self.dummy_gamma = self.gamma      \n",
    "        \n",
    "        # Warm-up las redes para evitar compilación durante entrenamiento\n",
    "        _ = self.main_network(self.dummy_state) #####, training=False)\n",
    "        _ = self.target_network(self.dummy_state) #####, training=False)\n",
    "        print(\"[INFO] Tensores pre-alocados y redes warm-up completado\")            \n",
    "            \n",
    "    def _crear_red(self, network_name):\n",
    "        \"\"\"\n",
    "        Construye una red individual con la arquitectura DDQN.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        network_name : str - Nombre identificador de la red ('main' o 'target').\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.keras.Sequential - Red neuronal construida.\n",
    "        \"\"\"\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv1'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv2'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                                 padding='same', activation='relu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv3'),\n",
    "            tf.keras.layers.Flatten(name=f'{network_name}_flatten'),\n",
    "            tf.keras.layers.Dense(units=256, activation='relu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_fc'),\n",
    "            tf.keras.layers.Dense(units=self.action_size, activation=None,\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_output')\n",
    "        ], name=f'{network_name}_network')\n",
    "\n",
    "    # Usa esto para evitar recompilaciones:\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=(None, 84, 84, 3), dtype=tf.float32)\n",
    "    ]) \n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante usando la red principal.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        training : bool    - Si está en modo entrenamiento (default: True)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red principal, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs, training=training)        \n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def get_target_q_values(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Obtiene los valores Q de la red objetivo.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red objetivo, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.target_network(inputs, training=training)\n",
    "    \n",
    "    def update_target_network(self, tau=None):\n",
    "        \"\"\"\n",
    "        Actualiza la red objetivo usando actualización suave (soft update).\n",
    "        \n",
    "        θ_target = τ * θ_main + (1 - τ) * θ_target\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        tau : float, opcional - Factor de actualización. Si es None, usa self.tau.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        main_weights = self.main_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "        \n",
    "        updated_weights = []\n",
    "        for main_w, target_w in zip(main_weights, target_weights):\n",
    "            updated_w = tau * main_w + (1 - tau) * target_w\n",
    "            updated_weights.append(updated_w)\n",
    "            \n",
    "        self.target_network.set_weights(updated_weights)   \n",
    "        # CRITICAL: Limpiar referencias para evitar memory leaks\n",
    "        del main_weights, target_weights, updated_weights        \n",
    "    \n",
    "    #@tf.function        \n",
    "    def train_step_optimized(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento DDQN.\n",
    "        \n",
    "        En DDQN:\n",
    "        1. La red principal selecciona la mejor acción para next_states\n",
    "        2. La red objetivo evalúa el valor Q de esa acción\n",
    "        3. Se calcula el target Q usando la ecuación de Bellman\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        states : tf.Tensor     - Estados actuales, shape = (batch_size, *state_size)\n",
    "        actions : tf.Tensor    - Acciones tomadas (one-hot), shape = (batch_size, action_size)\n",
    "        rewards : tf.Tensor    - Recompensas obtenidas, shape = (batch_size,)\n",
    "        next_states : tf.Tensor- Siguientes estados, shape = (batch_size, *state_size)\n",
    "        dones : tf.Tensor      - Flags de episodio terminado, shape = (batch_size,)\n",
    "        gamma : float          - Factor de descuento (default: 0.99)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor - Valor de la pérdida calculada en este paso.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Valores Q actuales de la red principal\n",
    "            current_q_values = self.main_network(states, training=True)\n",
    "            current_q_action = tf.reduce_sum(current_q_values * actions, axis=1)\n",
    "            \n",
    "            # DDQN: Red principal selecciona acciones, red objetivo las evalúa\n",
    "            next_q_values_main = self.main_network(next_states)  #####, training=True)\n",
    "            next_actions = tf.one_hot(tf.argmax(next_q_values_main, axis=1), self.action_size)\n",
    "            \n",
    "            next_q_values_target = self.target_network(next_states)  #####, training=True)\n",
    "            next_q_action = tf.reduce_sum(next_q_values_target * next_actions, axis=1)\n",
    "            \n",
    "            # Calcular target Q usando ecuación de Bellman\n",
    "            target_q = rewards + gamma * next_q_action * (1.0 - tf.cast(dones, tf.float32))\n",
    "            \n",
    "            # Calcular pérdida MSE\n",
    "            loss = self.loss_fn(target_q, current_q_action)\n",
    "        \n",
    "        # Aplicar gradientes solo a la red principal\n",
    "        gradients = tape.gradient(loss, self.main_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))\n",
    "        \n",
    "        return loss    \n",
    "    \n",
    "\n",
    "    def train_from_memory(self, batch_size, gamma):\n",
    "        \"\"\"Entrenar desde memoria con sampling optimizado.\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return None\n",
    "            \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Convertir a arrays numpy primero (más eficiente)\n",
    "        states = np.array([e[0] for e in batch], dtype=np.float32)\n",
    "        actions = np.array([e[1] for e in batch], dtype=np.int32)\n",
    "        rewards = np.array([e[2] for e in batch], dtype=np.float32)\n",
    "        next_states = np.array([e[3] for e in batch], dtype=np.float32)\n",
    "        dones = np.array([e[4] for e in batch], dtype=bool)\n",
    "        \n",
    "        # Convertir a tensores una sola vez\n",
    "        states_tf = tf.convert_to_tensor(states)\n",
    "        actions_onehot = tf.one_hot(actions, self.action_size)\n",
    "        rewards_tf = tf.convert_to_tensor(rewards)\n",
    "        next_states_tf = tf.convert_to_tensor(next_states)\n",
    "        dones_tf = tf.convert_to_tensor(dones)\n",
    "        \n",
    "        # Entrenar usando función optimizada\n",
    "        loss = self.train_step_optimized(states_tf, actions_onehot, rewards_tf, \n",
    "                                       next_states_tf, dones_tf, gamma)\n",
    "        return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNetworkWithReplay(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Double Deep Q-Network (DDQN) con memoria de repetición para mejorar la estabilidad del aprendizaje.\n",
    "\n",
    "    Esta clase extiende la funcionalidad de DDQN (como en `DDQNetwork`) al incorporar una memoria de repetición\n",
    "    (`ReplayMemory`) que almacena transiciones (estado, acción, recompensa, siguiente estado, done) y permite\n",
    "    muestrear lotes aleatorios para el entrenamiento. Esto rompe la correlación temporal entre experiencias\n",
    "    consecutivas, mejorando la eficiencia y estabilidad del aprendizaje en el entorno Space Invaders.\n",
    "    - Añade un búfer de memoria (`self.memory`) para almacenar transiciones.\n",
    "    - Incluye métodos `store_transition` y `train_from_memory` para gestionar la memoria de repetición.\n",
    "    - El entrenamiento puede usar lotes muestreados de la memoria en lugar de transiciones individuales.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    state_size : tupla/lista - Dimensiones del estado de entrada (e.g., [84, 84, 4] para 4 frames de 84x84).\n",
    "    action_size : int        - Número de acciones posibles en el entorno (e.g., 6 para Space Invaders).\n",
    "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
    "    memory_size : int        - Capacidad del búfer de memoria de repetición (default: 20000).\n",
    "    tau : float              - Factor de actualización suave para la red objetivo (default: 0.001).\n",
    "    name : str               - Nombre del modelo para identificación (default: 'DDQNetworkWithReplay').\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, learning_rate, memory_size = 5000, tau=0.001, name='DDQNetworkWithReplay'):    \n",
    "        # Inicializa la clase base tf.keras.Model con el nombre proporcionado.\n",
    "        # El nombre ayuda a identificar el modelo en logs o al guardar pesos. \n",
    "        super(DDQNetworkWithReplay, self).__init__(name=name)\n",
    "     \n",
    "        # Almacena los hiperparámetros básicos, idénticos a `DDQNetwork`.\n",
    "        # - state_size: Forma de los estados (e.g., [84, 84, 4] para una pila de 4 frames).\n",
    "        # - action_size: Número de acciones posibles (e.g., 6 para Space Invaders).\n",
    "        # - learning_rate: Tasa de aprendizaje para el optimizador (e.g., 0.00025).\n",
    "        # - memory_size : Tamaño máximo del búfer de memoria (default: 5000).        \n",
    "        # - tau: Factor para la actualización suave de la red objetivo (e.g., 0.001).        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "\n",
    "        # Inicializa el búfer de memoria de repetición con la capacidad especificada.\n",
    "        # Permite almacenar hasta `memory_size` transiciones (estado, acción, recompensa, siguiente estado, done).\n",
    "        # DEBUG: Imprime antes de inicializar self.memory\n",
    "        print(f\"[DEBUG - DDQNetworkWithReplay __init__] Inicializando self.memory...\")\n",
    "        self.memory = ReplayMemory(capacity=memory_size)\n",
    "        print(f\"[DEBUG - DDQNetworkWithReplay __init__] Tipo de self.memory: {type(self.memory)}\")\n",
    "        print(f\"[DEBUG - DDQNetworkWithReplay __init__] Métodos de self.memory: {[method for method in dir(self.memory) if not method.startswith('_')]}\")\n",
    "\n",
    "        \n",
    "        # Crear redes principal y objetivo, idénticas:\n",
    "        # - main_network: Selecciona acciones y se entrena activamente.\n",
    "        # - target_network: Evalúa valores Q objetivo para estabilidad.\n",
    "        self.main_network = self._crear_red('ddqn_rply_main')\n",
    "        self.target_network = self._crear_red('ddqn_rply_target')\n",
    "        \n",
    "        # Construir ambas redes con la forma de entrada correcta (e.g., (None, 84, 84, 4)).\n",
    "        # El `None` permite lotes de tamaño variable. Esto asegura que los pesos\n",
    "        # se inicialicen correctamente antes del entrenamiento.\n",
    "        # input_shape = (None,) + tuple(state_size)  # (None, 84, 84, 4)\n",
    "        # self.main_network.build(input_shape)\n",
    "        # self.target_network.build(input_shape)  \n",
    "        \n",
    "        # Pre-construir redes para evitar overhead durante entrenamiento\n",
    "        entrada_dummy = tf.zeros((1,) + tuple(state_size), dtype=tf.float32)\n",
    "        self.main_network(entrada_dummy)\n",
    "        self.target_network(entrada_dummy)           \n",
    "        \n",
    "        # Definir optimizador y función de pérdida\n",
    "        # Configura el optimizador Adam y la pérdida MSE, usados en `train_step`.\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        # Inicializar la red objetivo con los mismos pesos que la principal\n",
    "        # Copia los pesos de `main_network` a `target_network` al inicio (tau=1.0 significa\n",
    "        # copia completa). Esto asegura que ambas redes comiencen idénticas.\n",
    "        self.update_target_network(tau=1.0)\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def memory(self):\n",
    "        \"\"\"Propiedad para acceder a la memoria de repetición de forma segura.\"\"\"\n",
    "        if not hasattr(self, '_replay_memory'):\n",
    "            raise AttributeError(\"La memoria de repetición no ha sido inicializada correctamente\")\n",
    "        return self._replay_memory\n",
    "    \n",
    "    @memory.setter\n",
    "    def memory(self, value):\n",
    "        \"\"\"Setter para la propiedad memory - previene sobrescritura accidental.\"\"\"\n",
    "        if not isinstance(value, ReplayMemory):\n",
    "            raise TypeError(f\"memory debe ser una instancia de ReplayMemory, no {type(value)}\")\n",
    "        self._replay_memory = value        \n",
    "        \n",
    "    def _crear_red(self, network_name):\n",
    "        \"\"\"\n",
    "        Construye una red neuronal convolucional para DDQN.\n",
    "        La arquitectura es estándar para juegos de Atari:\n",
    "        - 3 capas convolucionales (32, 64, 64 filtros) con kernels 8x8, 4x4, 3x3.\n",
    "        - 1 capa densa de 512 unidades.\n",
    "        - Capa de salida con `action_size` unidades (valores Q).\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        network_name : str - Identificador para nombrar las capas ('main' o 'target').\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.keras.Sequential - Red neuronal construida.\n",
    "        \"\"\"\n",
    "        # - Conv2D: Extrae características espaciales de los frames.\n",
    "        # - strides: Reducen la dimensionalidad (downsampling).\n",
    "        # - padding='same': Mantiene el tamaño espacial.\n",
    "        # - activation='elu': Mitiga problemas de gradientes (mejor que ReLU).\n",
    "        # - glorot_uniform: Inicialización estándar para redes profundas.\n",
    "        # - Flatten: Convierte la salida convolucional en un vector.\n",
    "        # - Dense(512): Combina características para aprendizaje complejo.\n",
    "        # - Dense(action_size): Salida lineal para valores Q por acción.        \n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4),\n",
    "                                 padding='same', activation='elu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv1'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=(2,2),\n",
    "                                 padding='same', activation='elu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv2'),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),\n",
    "                                 padding='same', activation='elu',\n",
    "                                 kernel_initializer='glorot_uniform',\n",
    "                                 name=f'{network_name}_conv3'),\n",
    "            tf.keras.layers.Flatten(name=f'{network_name}_flatten'),\n",
    "            tf.keras.layers.Dense(units=256, activation='elu',\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_fc'),\n",
    "            tf.keras.layers.Dense(units=self.action_size, activation=None,\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                name=f'{network_name}_output')\n",
    "        ], name=f'{network_name}_network')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"\n",
    "        Propagación hacia adelante usando la red principal.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size)\n",
    "        training : bool    - Si está en modo entrenamiento (default: True - se usa \n",
    "                             principalmente para entrenamiento, donde se calculan \n",
    "                             gradientes, pero se puede sobrescribir a False para inferencia\n",
    "                            (e.g., selección de acciones en `simple_train`).\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red principal, shape = (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        return self.main_network(inputs, training=training)        \n",
    "    \n",
    "    def get_target_q_values(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Obtiene los valores Q de la red objetivo.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        inputs : tf.Tensor - Estados de entrada, shape = (batch_size, *state_size).\n",
    "        training : bool    - Modo entrenamiento (True) o inferencia (False).\n",
    "                             (default: False - Esto asegura que la red objetivo opere \n",
    "                             en modo inferencia, proporcionando valores Q estables \n",
    "                             sin calcular gradientes.        \n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.Tensor - Valores Q de la red objetivo, shape = (batch_size, action_size).\n",
    "\n",
    "        Con `training=False` fijo (ver tu pregunta\n",
    "        anterior). Esto asegura que la red objetivo opere en modo inferencia, proporcionando\n",
    "        valores Q estables sin calcular gradientes.\n",
    "        \"\"\"\n",
    "        return self.target_network(inputs, training=training)\n",
    "    \n",
    "    def update_target_network(self, tau=None):\n",
    "        \"\"\"\n",
    "        Actualiza los pesos de la red objetivo con una actualización suave:\n",
    "        θ_target = τ * θ_main + (1 - τ) * θ_target\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        tau : float, opcional - Factor de actualización (default: self.tau).\n",
    "              Usa `tau=1.0` al inicio para copia completa, y `tau=0.001` durante el \n",
    "              entrenamiento para actualizaciones graduales, estabilizando el aprendizaje.\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        main_weights = self.main_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "        \n",
    "        updated_weights = []\n",
    "        for main_w, target_w in zip(main_weights, target_weights):\n",
    "            updated_w = tau * main_w + (1 - tau) * target_w\n",
    "            updated_weights.append(updated_w)\n",
    "            \n",
    "        self.target_network.set_weights(updated_weights)    \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Almacena una transición en la memoria de repetición. Este método es exclusivo de \n",
    "        `DDQNetworkWithReplay`. Permite guardar transiciones en `self.memory` para su uso \n",
    "        posterior en `train_from_memory`.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        state : np.ndarray - Estado actual.\n",
    "        action : int       - Acción tomada.\n",
    "        reward : float     - Recompensa obtenida.\n",
    "        next_state : np.ndarray - Siguiente estado.\n",
    "        done : bool        - Indica si el episodio terminó.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            memory_obj = self.memory  # Usa la propiedad\n",
    "            if not hasattr(memory_obj, 'push'):\n",
    "                print(f\"[ERROR] memory no tiene método 'push'. Tipo actual: {type(memory_obj)}\")\n",
    "                print(f\"[ERROR] Métodos disponibles: {[method for method in dir(memory_obj) if not method.startswith('_')]}\")\n",
    "                print(f\"[ERROR] Atributos de self: {[attr for attr in dir(self) if 'memory' in attr.lower()]}\")\n",
    "                raise AttributeError(f\"memory (tipo: {type(memory_obj)}) no tiene método 'push'\")\n",
    "            \n",
    "            memory_obj.push(state, action, reward, next_state, done)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error en store_transition: {e}\")\n",
    "            print(f\"[ERROR] Tipo de self: {type(self)}\")\n",
    "            print(f\"[ERROR] Atributos de self relacionados con memory: {[attr for attr in dir(self) if 'memory' in attr.lower()]}\")\n",
    "            raise\n",
    "    \n",
    "    @tf.function        \n",
    "    def train_step(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Realiza un paso de entrenamiento DDQN.\n",
    "        \n",
    "        Calcula la pérdida usando la ecuación de Bellman para DDQN:\n",
    "        1. La red principal selecciona la acción óptima para next_states.\n",
    "        2. La red objetivo evalúa el valor Q de esa acción.\n",
    "        3. Target Q = reward + γ * Q_target(next_state, argmax(Q_main(next_state))).\n",
    "        Usa `@tf.function` para optimizar la ejecución compilando el método en un grafo de \n",
    "        TensorFlow, mejorando el rendimiento.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        states : tf.Tensor     - Estados actuales, shape = (batch_size, *state_size)\n",
    "        actions : tf.Tensor    - Acciones tomadas (one-hot), shape = (batch_size, action_size)\n",
    "        rewards : tf.Tensor    - Recompensas obtenidas, shape = (batch_size,)\n",
    "        next_states : tf.Tensor- Siguientes estados, shape = (batch_size, *state_size)\n",
    "        dones : tf.Tensor      - Flags de episodio terminado, shape = (batch_size,)\n",
    "        gamma : float          - Factor de descuento (default: 0.99)\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor - Valor de la pérdida calculada en este paso.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Valores Q actuales de la red principal\n",
    "            current_q_values = self.main_network(states, training=True)\n",
    "            current_q_action = tf.reduce_sum(current_q_values * actions, axis=1)\n",
    "            \n",
    "            # DDQN: Red principal selecciona acciones, red objetivo las evalúa\n",
    "            next_q_values_main = self.main_network(next_states, training=True)\n",
    "            next_actions = tf.one_hot(tf.argmax(next_q_values_main, axis=1), self.action_size)\n",
    "            \n",
    "            next_q_values_target = self.target_network(next_states, training=False)\n",
    "            next_q_action = tf.reduce_sum(next_q_values_target * next_actions, axis=1)\n",
    "            \n",
    "            # Calcular target Q usando ecuación de Bellman\n",
    "            target_q = rewards + gamma * next_q_action * (1.0 - tf.cast(dones, tf.float32))\n",
    "            \n",
    "            # Calcular pérdida MSE\n",
    "            loss = self.loss_fn(target_q, current_q_action)\n",
    "        \n",
    "        # Aplicar gradientes solo a la red principal\n",
    "        gradients = tape.gradient(loss, self.main_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))\n",
    "        \n",
    "        return loss   \n",
    "\n",
    "    def train_from_memory(self, batch_size, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Entrena la red usando un lote aleatorio muestreado de la memoria de repetición. Este \n",
    "        método es exclusivo de `DDQNetworkWithReplay`:\n",
    "        - Muestrea `batch_size` transiciones de `self.memory`.\n",
    "        - Convierte las transiciones a tensores para usarlas en `train_step`.\n",
    "        - Permite entrenar con experiencias pasadas, mejorando la estabilidad.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        batch_size : int  - Tamaño del lote a muestrear (e.g., 32).\n",
    "        gamma : float     - Factor de descuento (default: 0.99).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        loss : tf.Tensor or None - Pérdida calculada, o None si no hay suficientes transiciones.\n",
    "        \"\"\"\n",
    "        # Verificar que hay suficientes transiciones\n",
    "        if not self.memory.can_sample(batch_size):\n",
    "            print(f\"[WARNING] No hay suficientes transiciones en memoria. Actual: {len(self.memory)}, Requerido: {batch_size}\")\n",
    "            return None\n",
    "           \n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        states = tf.convert_to_tensor(np.array(states), dtype=tf.float32)\n",
    "        actions = tf.stack([tf.one_hot(a, self.action_size) for a in actions])\n",
    "        #actions = tf.convert_to_tensor(np.array([tf.one_hot(a, self.action_size) for a in actions]), dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(np.array(rewards), dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(np.array(next_states), dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(np.array(dones), dtype=tf.float32)\n",
    "        loss = self.train_step(states, actions, rewards, next_states, dones, gamma)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQNetworkWithReplay(DDQNetworkWithReplay):\n",
    "    \"\"\"\n",
    "    Dueling Double Deep Q-Network (Dueling DDQN) con memoria de repetición para aprendizaje por refuerzo.\n",
    "\n",
    "    Esta clase extiende `DDQNetworkWithReplay` para implementar una arquitectura dueling, que separa la estimación\n",
    "    del valor del estado (`V(s)`) y la ventaja de las acciones (`A(s,a)`) en la red neuronal, mejorando la precisión\n",
    "    de los valores Q. Está diseñada para el entorno *SpaceInvaders-v0* de OpenAI Gym, integrándose con el pipeline\n",
    "    de entrenamiento (`simple_train`, `crear_modelo`, `AtariProcessor`). La memoria de repetición y el mecanismo\n",
    "    de Double DQN aseguran estabilidad y eficiencia en el aprendizaje.\n",
    "    \n",
    "    Características principales:\n",
    "    - Arquitectura dueling: Divide la red en flujos de valor y ventaja, combinados como `Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))`.\n",
    "    - Double DQN: Usa la red principal para seleccionar acciones y la red objetivo para evaluarlas, reduciendo la sobreestimación.\n",
    "    - Memoria de repetición: Almacena transiciones (estado, acción, recompensa, siguiente estado, done) y muestrea lotes aleatorios.\n",
    "    - Actualización suave de la red objetivo con factor `tau`.\n",
    "    - Optimizada para frames preprocesados de Atari (84x84 en escala de grises, apilados).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, learning_rate, memory_size=5000, tau=0.001, name=\"DuelingDQNWithReplay\"):\n",
    "        \"\"\"\n",
    "        Inicializa la red Dueling DDQN con memoria de repetición.\n",
    "\n",
    "        Configura los hiperparámetros, inicializa la memoria de repetición, y crea las redes principal y objetivo\n",
    "        usando la arquitectura dueling definida en `build_model`. Hereda la funcionalidad de `DDQNetworkWithReplay`\n",
    "        para la gestión de la memoria y el entrenamiento.\n",
    "\n",
    "        Parámetros:\n",
    "        -----------\n",
    "        state_size : tuple/list - Forma de los estados (e.g., [3, 84, 84] para una pila de 3 frames).\n",
    "        action_size : int       - Número de acciones posibles (e.g., 6 para *SpaceInvaders-v0*).\n",
    "        learning_rate : float   - Tasa de aprendizaje para el optimizador Adam (e.g., 0.00025).\n",
    "        memory_size : int       - Tamaño máximo del búfer de memoria (default: 5000).\n",
    "        tau : float             - Factor para la actualización suave de la red objetivo (default: 0.001).\n",
    "        name : str              - Identificador del modelo (default: 'DuelingDQNWithReplay').\n",
    "        \"\"\"\n",
    "        # Inicializa la clase padre `DDQNetworkWithReplay` con los parámetros proporcionados.\n",
    "        # Esto configura la memoria de repetición (`ReplayMemory`), las redes principal y objetivo,\n",
    "        # el optimizador, y la función de pérdida, heredando métodos como `train_step` y `update_target_network`.\n",
    "        super().__init__(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau, name=name)\n",
    "\n",
    "    def _crear_red(self, name):\n",
    "        \"\"\"\n",
    "        Construye la red neuronal Dueling DDQN para estimar valores Q.\n",
    "\n",
    "        Define una arquitectura dueling con capas convolucionales para procesar frames de Atari, seguida de\n",
    "        dos flujos separados: uno para el valor del estado (`V(s)`) y otro para la ventaja de las acciones (`A(s,a)`).\n",
    "        Los flujos se combinan para producir los valores Q: `Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))`.\n",
    "        La red es compilada con el optimizador Adam y la pérdida de error cuadrático medio (MSE).\n",
    "\n",
    "        Retorna:\n",
    "        --------\n",
    "        tf.keras.Model - Red neuronal compilada que mapea estados a valores Q.\n",
    "        \"\"\"\n",
    "        # Define la forma de entrada basada en `state_size` (e.g., [84, 84, 3] para 3 frames apilados).\n",
    "        # La entrada espera frames de 84x84 píxeles en escala de grises con `WINDOW_LENGTH` canales.\n",
    "        input_shape = tuple(self.state_size)  # Correcto: (84, 84, 3)\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # Capas convolucionales para extraer características espaciales de los frames.\n",
    "        # - Conv2D: Filtros de tamaño creciente (32, 64, 64) con kernels 8x8, 4x4, 3x3.\n",
    "        # - Strides: Reducen la dimensionalidad (downsampling) para eficiencia computacional.\n",
    "        # - Activación ReLU: Introduce no linealidad para modelar patrones complejos.\n",
    "        # - Padding='valid': No agrega relleno, reduciendo el tamaño de salida.\n",
    "        x = Conv2D(16, (8, 8), strides=4, activation='relu', padding='valid',\n",
    "                                 name=f'Dueling_{name}_conv1')(inputs)\n",
    "        x = Conv2D(32, (4, 4), strides=2, activation='relu', padding='valid',\n",
    "                                 name=f'Dueling_{name}_conv2')(x)\n",
    "        x = Conv2D(32, (3, 3), strides=1, activation='relu', padding='valid',\n",
    "                                 name=f'Dueling_{name}_conv3')(x)\n",
    "        x = Flatten(name=f'Dueling_{name}_flatten')(x)  # Convierte la salida convolucional en un vector 1D.\n",
    "        \n",
    "        # Flujo de valor (`V(s)`): Estima cuán bueno es estar en un estado.\n",
    "        # - Dense(256): Capa densa para combinar características.\n",
    "        # - Dense(1): Salida escalar que representa el valor del estado.\n",
    "        value_fc = Dense(256, activation='ReLU')(x)\n",
    "        value = Dense(1, activation='linear', name=f'Dueling_{name}_value')(value_fc)\n",
    "        \n",
    "        # Flujo de ventaja (`A(s,a)`): Estima la ventaja relativa de cada acción.\n",
    "        # - Dense(256): Capa densa para combinar características.\n",
    "        # - Dense(action_size): Salida vectorial con una ventaja por acción.\n",
    "        advantage_fc = Dense(256, activation='ReLU')(x)\n",
    "        advantage = Dense(self.action_size, activation='linear', name=f'Dueling_{name}_advantage')(advantage_fc)\n",
    "        \n",
    "        # Combinación dueling: Calcula Q-values restando la media de las ventajas.\n",
    "        # - Lambda: Calcula la media de las ventajas sobre el eje de acciones.\n",
    "        # - Lambda: Resta la media para centrar las ventajas.\n",
    "        # - Add: Combina el valor y las ventajas centradas: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a))).\n",
    "        advantage_mean = Lambda(lambda a: K.mean(a, axis=1, keepdims=True))(advantage)\n",
    "        q_values = Add()([value, Lambda(lambda a: a - advantage_mean)(advantage)])\n",
    "        \n",
    "        # Crea el modelo Keras que mapea estados a valores Q.\n",
    "        model = Model(inputs=inputs, outputs=q_values)\n",
    "        \n",
    "        return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "### 2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear clases a prefijos\n",
    "class_to_prefix = {\n",
    "        DQNetwork: \"DQN\",\n",
    "        DDQNetwork: \"DDQN\",\n",
    "        DDQNetworkWithReplay: \"DDQN_Replay\",\n",
    "        DuelingDQNetworkWithReplay: \"DuelingDQN_Replay\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state, processor, frame_stack):\n",
    "    \"\"\"\n",
    "    Preprocesa una observación del entorno para generar un estado apilado de fotogramas.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------  \n",
    "        observation (np.ndarray):   Observación cruda del entorno (un fotograma RGB).\n",
    "        processor (AtariProcessor): Objeto procesador que convierte la observación a escala de grises y la redimensiona.\n",
    "        frame_stack (FrameStack):   Objeto que gestiona la pila de fotogramas para mantener el contexto temporal.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        np.ndarray: Estado preprocesado, consistente en una pila de fotogramas (shape: [84, 28, WINDOW_LENGTH]).\n",
    "    \"\"\"\n",
    "    processed_frame = processor.process_observation(state)\n",
    "    frame_stack.add_frame(processed_frame)\n",
    "    state = frame_stack.get_state()\n",
    "    return processor.process_state_batch(np.expand_dims(state, 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_memory(dqnet, dqnet_class, class_to_prefix, checkpoint_path=None, memory_path=None, memory_size=2000):\n",
    "    \"\"\"\n",
    "    Carga los pesos del modelo y, si aplica, la memoria de repetición, con soporte para buscar el último checkpoint.\n",
    "\n",
    "    Esta función carga los pesos de un modelo de red neuronal desde un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), carga la memoria desde un archivo pickle (.pkl).\n",
    "    Si `use_latest_checkpoint` es True, busca automáticamente el checkpoint y memoria más recientes en `checkpoint_dir`\n",
    "    basándose en el número de episodio en el nombre del archivo.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        dqnet: Objeto del modelo de red neuronal, como una instancia de DQNetwork, DDQNetwork, DDQNetworkWithReplay o\n",
    "            DuelingDQNetworkWithReplay.\n",
    "        dqnet_class: Clase del modelo (e.g., DQNetwork, DDQNetwork, DDQNetworkWithReplay, DuelingDQNetworkWithReplay).\n",
    "            Se usa para verificar si el modelo soporta memoria de repetición y para determinar el prefijo de los\n",
    "            nombres de archivo de checkpoint.\n",
    "        class_to_prefix (dict): Diccionario que mapea clases de modelos a prefijos (e.g., {DQNetwork: 'DQN'}).\n",
    "        checkpoint_path: str, opcional. Directorio donde se buscan o guardan los archivos de checkpoint y memoria\n",
    "            (e.g., 'checkpoints'). Usado cuando `use_latest_checkpoint` es True o para derivar rutas si\n",
    "            `checkpoint_path`/`memory_path` no están especificados. Por defecto es \"checkpoints\".\n",
    "        memory_path: str, opcional. Ruta completa al archivo .pkl que contiene la memoria de repetición (e.g.,\n",
    "            'checkpoints/DuelingDQN_Replay_memory_ep10.pkl'). Solo aplica a modelos con replay. Si se proporciona,\n",
    "            tiene prioridad sobre la búsqueda automática. Por defecto es None.\n",
    "        memory_size: int, opcional. Tamaño máximo de la memoria de repetición (número de transiciones almacenadas).\n",
    "            Se usa para inicializar una memoria vacía si no se carga ninguna. Por defect\n",
    "\n",
    "    Returns:\n",
    "        int: El número de episodio correspondiente al checkpoint cargado (extraído del nombre del archivo, e.g., 10\n",
    "            para 'DQN_checkpoint_ep10.h5'). Retorna 0 si no se carga ningún checkpoint o si falla la carga.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si `dqnet_class` no está en el mapeo interno de clases a prefijos.\n",
    "        Exception: Captura y registra errores durante la búsqueda o carga de archivos, retornando 0 en caso de fallo.\n",
    "    \"\"\"\n",
    "    if dqnet_class not in class_to_prefix:\n",
    "        print(f\"[ERROR] - Clase {dqnet_class.__name__} no soportada\")\n",
    "        return 0\n",
    "    \n",
    "    prefijo = class_to_prefix[dqnet_class]    \n",
    "    checkpoint_pattern = re.compile(rf'^{prefijo}_checkpoint_ep(\\d+)\\.h5$')\n",
    "    memory_pattern = re.compile(rf'^{prefijo}_memory_ep(\\d+)\\.pkl$')\n",
    "    latest_checkpoint = None\n",
    "    latest_memory = None    \n",
    "    latest_episode = 0\n",
    "\n",
    "    try:\n",
    "        # Asegurar que el directorio existe\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f\"[INFO] - Directorio {checkpoint_path} no existe, comenzando desde cero\")\n",
    "            return 0\n",
    "\n",
    "        # Buscar en el directorio\n",
    "        for file in os.listdir(checkpoint_path):\n",
    "            # Buscar checkpoints\n",
    "            checkpoint_match = checkpoint_pattern.match(file)\n",
    "            if checkpoint_match:\n",
    "                episode = int(checkpoint_match.group(1))\n",
    "                if episode > latest_episode:\n",
    "                    latest_episode = episode\n",
    "                    latest_checkpoint = os.path.join(checkpoint_path, file)\n",
    "\n",
    "        # Buscar memoria correspondiente al mismo episodio\n",
    "        if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay] and latest_episode > 0:\n",
    "            memory_file = f\"{prefijo}_memory_ep{latest_episode}.pkl\"\n",
    "            memory_path_candidate = os.path.join(checkpoint_path, memory_file)\n",
    "            if os.path.exists(memory_path_candidate):\n",
    "                latest_memory = memory_path_candidate\n",
    "\n",
    "        if latest_checkpoint:\n",
    "            print(f\"[INFO] - Último checkpoint encontrado: {latest_checkpoint} (episodio {latest_episode})\")\n",
    "            if latest_memory:\n",
    "                print(f\"[INFO] - Última memoria encontrada: {latest_memory} (episodio {latest_episode})\")\n",
    "        else:\n",
    "            print(f\"[INFO] - No se encontraron checkpoints para {prefijo} en {checkpoint_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] - Error buscando checkpoints: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    # Cargar pesos desde checkpoint si se proporciona\n",
    "    if latest_checkpoint and os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            \"\"\" \n",
    "            \n",
    "            import h5py\n",
    "\n",
    "            new_path = 'checkpoints\\DuelingDQN_Replay_checkpoint_ep10_renamed.h5'\n",
    "\n",
    "            with h5py.File(latest_checkpoint, 'r') as old_f:\n",
    "                with h5py.File(new_path, 'w') as new_f:\n",
    "                    for key in old_f.keys():\n",
    "                        if key == 'model': # Si el nombre es 'model'\n",
    "                          old_f.copy(key, new_f, name='model_1') # Cópialo con el nombre 'model_1'\n",
    "                        else:\n",
    "                            old_f.copy(key, new_f) # Copia los demás objetos tal cual\n",
    "                            \"\"\"\n",
    "            # Ahora intenta cargar desde new_path            \n",
    "            dqnet.main_network.load_weights(latest_checkpoint)\n",
    "            print(f\"[INFO] - Pesos cargados desde {latest_checkpoint}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] - No se pudieron cargar, desde {latest_checkpoint}, los pesos: {e}\")\n",
    "            latest_episode = 0  # Resetear episodio si falla            \n",
    "    else:\n",
    "        print(\"[INFO] - No se proporcionó checkpoint, comenzando desde cero\")\n",
    "\n",
    "    # Cargar memoria de repetición si se proporciona y es un modelo con replay\n",
    "    if latest_memory and os.path.exists(latest_memory) and dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "        try:\n",
    "            with open(latest_memory, 'rb') as f:\n",
    "                dqnet.memory = pickle.load(f)\n",
    "            print(f\"[INFO] - Memoria de repetición cargada desde {latest_memory}\")\n",
    "            print(f\"[INFO] - Tamaño de la memoria cargada: {len(dqnet.memory)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] - No se pudo cargar la memoria: {e}\")\n",
    "            dqnet.memory = ReplayMemory(capacity=memory_size)  # Reiniciar si falla\n",
    "    elif dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "        print(\"[INFO] - No se proporcionó memoria, inicializando memoria vacía\")\n",
    "        dqnet.memory = ReplayMemory(capacity=memory_size)  # Asegurar que la memoria esté inicializada\n",
    "        \n",
    "    return latest_episode        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_memory(dqnet, dqnet_class, prefijo, episode, checkpoint_path=\"checkpoints\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Guarda los pesos del modelo y, si aplica, la memoria de repetición en archivos.\n",
    "\n",
    "    Esta función guarda los pesos de un modelo de red neuronal en un archivo HDF5 (.h5) y, para modelos con memoria\n",
    "    de repetición (como DDQNetworkWithReplay o DuelingDQNetworkWithReplay), guarda la memoria en un archivo pickle (.pkl).\n",
    "    Los archivos se nombran usando un prefijo basado en la clase del modelo y el número de episodio, siguiendo el formato\n",
    "    `{prefijo}_checkpoint_ep{episode}.h5` para pesos y `{prefijo}_memory_ep{episode}.pkl` para memoria.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        dqnet: Objeto del modelo de red neuronal, como una instancia de DQNetwork, DDQNetwork, DDQNetworkWithReplay o\n",
    "            DuelingDQNetworkWithReplay. Debe tener un método `save_weights` para guardar pesos y, si usa memoria, un\n",
    "            atributo `memory`.\n",
    "        dqnet_class: Clase del modelo (e.g., DQNetwork, DDQNetwork, DDQNetworkWithReplay, DuelingDQNetworkWithReplay).\n",
    "            Se usa para determinar el prefijo del nombre de archivo y verificar si el modelo soporta memoria de repetición.\n",
    "        prefijo (str): Cadena que identifica el tipo de modelo y se usa como prefijo en los nombres de los archivos\n",
    "                       de checkpoint y memoria (e.g., 'DQN', 'DDQN', 'DDQN_Replay', 'DuelingDQN_Replay'). \n",
    "                       Se deriva del mapeo class_to_prefix basado en dqnet_class.            \n",
    "        episode: int. Número del episodio actual del entrenamiento. Se usa para nombrar los archivos de checkpoint y\n",
    "            memoria (e.g., `checkpoint_ep10.h5` para el episodio 10).\n",
    "        checkpoint_dir: str, opcional. Directorio donde se guardan los archivos de checkpoint y memoria\n",
    "            (e.g., 'checkpoints'). Se crea el directorio si no existe. Por defecto es \"checkpoints\".\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si `dqnet_class` no está en el mapeo interno de clases a prefijos.\n",
    "        Exception: Captura y registra errores durante la escritura de archivos, pero no interrumpe la ejecución.\n",
    "    \"\"\"\n",
    "    # Crear directorio de checkpoints si no existe\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "    memory_path_save = None\n",
    "\n",
    "    try:\n",
    "        # Guardar pesos del modelo\n",
    "        checkpoint_path_save = os.path.join(checkpoint_path, f'{prefijo}_checkpoint_ep{episode + 1}{suffix}.h5')\n",
    "        dqnet.main_network.save_weights(checkpoint_path_save)\n",
    "        print(f\"💾 Guardado: {checkpoint_path_save}\")\n",
    "\n",
    "        # Guardar memoria de repetición para modelos con replay\n",
    "        if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "            memory_path_save = os.path.join(checkpoint_path, f'{prefijo}_memory_ep{episode + 1}{suffix}.pkl')\n",
    "            try:\n",
    "                with open(memory_path_save, 'wb') as f:\n",
    "                    pickle.dump(dqnet.memory, f)\n",
    "                print(f\"💾 Memoria guardada: {memory_path_save}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error guardando memoria: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error guardando checkpoint: {e}\")\n",
    "\n",
    "    return checkpoint_path_save, memory_path_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in d:\\90-tools\\anaconda3\\envs\\mghmiar08\\lib\\site-packages (5.9.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con esto:\n",
    "def safe_clear_session():\n",
    "    \"\"\"Limpia la sesión de TensorFlow de forma segura\"\"\"\n",
    "    gc.collect()  # Primero recolectar objetos Python\n",
    "    \n",
    "    # Solo limpiar la sesión si no estamos en un contexto de grafo anidado\n",
    "    if tf.executing_eagerly():\n",
    "        tf.keras.backend.clear_session()\n",
    "    else:\n",
    "        print(\"Advertencia: No se puede limpiar la sesión dentro de un grafo anidado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_memory_cleanup():\n",
    "    \"\"\"Limpia memoria de forma segura sin depender de clear_session\"\"\"  \n",
    "    # Forzar recolección de objetos Python\n",
    "    gc.collect()\n",
    "    \n",
    "    # Intentar liberar caché de GPU si hay disponible\n",
    "    try:\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            for device in tf.config.list_physical_devices('GPU'):\n",
    "                tf.config.experimental.reset_memory_stats(device)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # No llamamos a clear_session() porque eso es lo que causa el error\n",
    "    print(\"Limpieza de memoria realizada (sin clear_session)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al final de cada paso o batch, añade:\n",
    "def cleanup_tensors():\n",
    "    \"\"\"Limpia tensores innecesarios\"\"\"\n",
    "    # Liberar variables temporales grandes\n",
    "    if 'tensor_cache' in globals():\n",
    "        globals()['tensor_cache'].clear()\n",
    "    \n",
    "    # Forzar liberación de memoria de GPU si está disponible\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        tf.keras.backend.clear_session()\n",
    "        # Esperar a que la GPU termine operaciones pendientes\n",
    "        if hasattr(tf, 'experimental'):\n",
    "            tf.experimental.sync_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 ** 2)  # Convertir a MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    \"\"\"Limpieza agresiva de memoria\"\"\"\n",
    "    # Forzar recolección de basura de Python\n",
    "    gc.collect()\n",
    "    \n",
    "    # Si no estamos en un contexto de grafo, limpiar sesión\n",
    "    if tf.executing_eagerly():\n",
    "        # Limpiar caché interna de TensorFlow\n",
    "        safe_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "# Al inicio de tu script principal\n",
    "import tracemalloc\n",
    "\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "# Suppress deprecated warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "####simple_train\n",
    "def simple_train(env, dqnet_class, processor, class_to_prefix, epsilon_start, total_episodios, max_steps, batch_size, gamma, memory_size=10000, tau=0.001, start_episode=0, checkpoint_path='checkpoints', memory_path=None):\n",
    "    # Limpiar cualquier grafo anterior\n",
    "    gc.collect()     \n",
    "    tf.config.run_functions_eagerly(True)    \n",
    "       \n",
    "    memory = ReplayMemory(memory_size, state_shape=(84, 84, 3))\n",
    "    dqnet = dqnet_class(state_size=(84, 84, 3), action_size=env.action_space.n, learning_rate=0.00025)\n",
    "    gamma_tf = tf.constant(gamma, dtype=tf.float32)\n",
    "    \n",
    "    state = env.reset()\n",
    "       \n",
    "    state = processor.process_observation(state)\n",
    "    # Ensure state has 3 channels\n",
    "    state = np.array(state)\n",
    "    if state.shape[-1] != 3:\n",
    "        state = np.repeat(state[..., np.newaxis], 3, axis=-1)\n",
    "    \n",
    "    for _ in range(min(memory_size // 10, 1000)):\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = processor.process_observation(next_state)\n",
    "        # Ensure next_state has 3 channels\n",
    "        next_state = np.array(next_state)\n",
    "        if next_state.shape[-1] != 3:\n",
    "            next_state = np.repeat(next_state[..., np.newaxis], 3, axis=-1)\n",
    "        memory.append(state, action, reward, next_state, done)\n",
    "        state = next_state if not done else processor.process_observation(env.reset())\n",
    "        # Ensure state has 3 channels after reset\n",
    "        state = np.array(state)\n",
    "        if state.shape[-1] != 3:\n",
    "            state = np.repeat(state[..., np.newaxis], 3, axis=-1)\n",
    "    \n",
    "    for episode in range(start_episode, total_episodios):\n",
    "             \n",
    "        # Iniciar temporizador y capturar memoria inicial\n",
    "        start_time = time.time()\n",
    "        initial_memory = psutil.Process().memory_info().rss / (1024 * 1024)  # MB\n",
    "    \n",
    "        print(f\"Episodio {episode+1}/{total_episodios} comenzando. Memoria inicial: {initial_memory:.2f} MB\")\n",
    "    \n",
    "    \n",
    "        gc.collect() \n",
    "        state = env.reset()\n",
    "        state = processor.process_observation(state)\n",
    "        # Ensure state has 3 channels\n",
    "        state = np.array(state)\n",
    "        if state.shape[-1] != 3:\n",
    "            state = np.repeat(state[..., np.newaxis], 3, axis=-1)\n",
    "        total_reward = 0     \n",
    "    \n",
    "        for step in range(max_steps):\n",
    "            start_time_e = time.time()  \n",
    "            cleanup_tensors()\n",
    "            epsilon = max(0.01, epsilon_start - (epsilon_start - 0.01) * episode / total_episodios)\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                state_array = np.array(state)\n",
    "                if state_array.shape[-1] != 3:\n",
    "                    state_array = np.repeat(state_array[..., np.newaxis], 3, axis=-1)\n",
    "                state_tensor = tf.expand_dims(tf.convert_to_tensor(state_array, dtype=tf.float32), axis=0)\n",
    "                q_values = dqnet.main_network(state_tensor, training=False)\n",
    "                action = tf.keras.backend.get_value(tf.argmax(q_values[0]))\n",
    "                del state_tensor, q_values\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = processor.process_observation(next_state)\n",
    "            # Ensure next_state has 3 channels\n",
    "            next_state = np.array(next_state)\n",
    "            if next_state.shape[-1] != 3:\n",
    "                next_state = np.repeat(next_state[..., np.newaxis], 3, axis=-1)\n",
    "            total_reward += reward\n",
    "            memory.append(state, action, reward, next_state, done)\n",
    "            state = next_state if not done else processor.process_observation(env.reset())\n",
    "            # Ensure state has 3 channels after reset\n",
    "            state = np.array(state)\n",
    "            if state.shape[-1] != 3:\n",
    "                state = np.repeat(state[..., np.newaxis], 3, axis=-1)\n",
    "            \n",
    "            if len(memory) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "                states = processor.process_state_batch(states)\n",
    "                next_states = processor.process_state_batch(next_states)\n",
    "                rewards = np.array(rewards, dtype=np.float32)\n",
    "                dones = np.array(dones, dtype=np.float32)\n",
    "                \n",
    "                states_tf = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "                actions_onehot = tf.convert_to_tensor(np.eye(env.action_space.n)[np.array(actions)], dtype=tf.float32)\n",
    "                rewards_tf = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "                next_state_tf = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "                dones_tf = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "                \n",
    "                loss = dqnet.train_step_optimized(states_tf, actions_onehot, rewards_tf, next_state_tf, dones_tf, gamma_tf)\n",
    "\n",
    "                del states_tf,actions_onehot,rewards_tf,next_state_tf,dones_tf\n",
    "\n",
    "                # Imprimir uso de memoria antes del entrenamiento del minibatch\n",
    "                print(f\"Uso de memoria antes del minibatch (episodio {episode}, paso {step}): {get_memory_usage():.2f} MB\")\n",
    "    \n",
    "            # CRUCIAL: Limpia variables grandes después de usarlas\n",
    "            if step % 25 == 0 and step > 0:\n",
    "                clean_memory()\n",
    "    \n",
    "            print(f\"⏱ Entrenamiento step: {time.time() - start_time_e:.3f} s\")   \n",
    "            if step % 100 == 0:\n",
    "                gc.collect()\n",
    "                print(f\"⏱ Entrenamiento 100: {time.time() - start_time:.3f} s\")  \n",
    "                print(f\"Step {step}, Loss: { tf.keras.backend.get_value(loss):.4f}\")\n",
    "                current, peak = tracemalloc.get_traced_memory()\n",
    "                print(f\"Memoria actual: {current / 10**6}MB; Pico: {peak / 10**6}MB\")                \n",
    "                start = time.time()  \n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Al final del episodio, antes de pasar al siguiente:\n",
    "    \n",
    "        # 1. Forzar limpieza de memoria\n",
    "        # Desreferenciar variables grandes\n",
    "        states_buffer = None\n",
    "        actions_buffer = None\n",
    "        # Forzar recolección de basura\n",
    "        gc.collect()        \n",
    "\n",
    "        # 2. Cerrar todas las figuras matplotlib\n",
    "        plt.close('all')\n",
    "\n",
    "        # 3. Intentar limpiar la sesión de forma segura\n",
    "        try:\n",
    "            safe_clear_session()\n",
    "        except Exception as e:\n",
    "            print(f\"No se pudo limpiar la sesión: {e}\")\n",
    "\n",
    "        # 4. Medir tiempo y memoria final\n",
    "        end_time = time.time()\n",
    "        final_memory = psutil.Process().memory_info().rss / (1024 * 1024)  # MB\n",
    "        episode_duration = end_time - start_time\n",
    "\n",
    "        # 5. Guardar métricas\n",
    "        episode_times.append(episode_duration)\n",
    "        memory_usage.append(final_memory)\n",
    "\n",
    "        print(f\"Episodio {episode+1} completado en {episode_duration:.2f} segundos\")\n",
    "        print(f\"Memoria final: {final_memory:.2f} MB (cambio: {final_memory - initial_memory:.2f} MB)\")\n",
    "\n",
    "        # 6. Cada 5 episodios, guardar diagnóstico de memoria\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(episode_times)\n",
    "            plt.title('Tiempo por episodio')\n",
    "            plt.ylabel('Segundos')\n",
    "            plt.xlabel('Episodio')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(memory_usage)\n",
    "            plt.title('Uso de memoria')\n",
    "            plt.ylabel('MB')\n",
    "            plt.xlabel('Episodio')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"diagnosticos/rendimiento_episodio_{episode+1}.png\")\n",
    "            plt.close()\n",
    "                 \n",
    "        # Limpieza agresiva entre episodios\n",
    "        clean_memory()                \n",
    "                \n",
    "        # Cada X episodios, guardar pesos, recrear el modelo y cargar pesos\n",
    "        if episode % 1 == 0 and episode > 0:            \n",
    "            # Para guardar el estado completo del modelo DDQN\n",
    "            main_weights = dqnet.main_network.get_weights()\n",
    "            target_weights = dqnet.target_network.get_weights()\n",
    "            # Reiniciar TensorFlow (elimina grafos acumulados)\n",
    "            safe_memory_cleanup()\n",
    "            \n",
    "            # Guardar ambos conjuntos de pesos\n",
    "            weights_to_save = {\n",
    "                'main': main_weights,\n",
    "                'target': target_weights\n",
    "            }\n",
    "\n",
    "            # Recrear completamente el modelo\n",
    "            del dqnet\n",
    "            gc.collect()\n",
    "            dqnet = dqnet_class(state_size=(84, 84, 3), action_size=env.action_space.n, learning_rate=0.00025)\n",
    "\n",
    "            # Para restaurar\n",
    "            dqnet.main_network.set_weights(weights_to_save['main'])\n",
    "            dqnet.target_network.set_weights(weights_to_save['target'])                 \n",
    "            # Limpiar referencias antiguas\n",
    "            del main_weights, target_weights\n",
    "            clean_memory()        \n",
    "        \n",
    "        # Cerrar y recrear el entorno\n",
    "        env.close()\n",
    "        env = gym.make('SpaceInvaders-v0', obs_type='rgb')\n",
    "        env.seed(123)   \n",
    "            \n",
    "        print(f\"Episodio {episode}, Recompensa total: {total_reward}\")\n",
    "        print(f\"⏱ Entrenamiento Episodio {episode}, Tiempo total: {time.time() - start:.3f} s\")   \n",
    "            \n",
    "        # Usa este enfoque (procesa cada peso individualmente):\n",
    "        target_weights = dqnet.target_network.get_weights()\n",
    "        main_weights = dqnet.main_network.get_weights()\n",
    "        updated_weights = []\n",
    "        for i in range(len(target_weights)):\n",
    "            updated_weights.append((1 - tau) * target_weights[i] + tau * main_weights[i])\n",
    "        dqnet.target_network.set_weights(updated_weights)       \n",
    "    \n",
    "    return dqnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train2(env,\n",
    "                 dqnet_class, \n",
    "                 processor, \n",
    "                 class_to_prefix, \n",
    "                 epsilon_start,\n",
    "                 total_episodios,\n",
    "                 max_steps, \n",
    "                 batch_size, \n",
    "                 gamma, \n",
    "                 memory_size=2000, \n",
    "                 tau=0.001,\n",
    "                 start_episode=0,                  # Episodio desde el cual retomar\n",
    "                 checkpoint_path=checkpoint_path,  # Ruta al archivo de checkpoint\n",
    "                 memory_path=None                  # Ruta al archivo de memoria                 \n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un modelo DQN en el entorno especificado con soporte para reanudar desde un checkpoint.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        env : gym.Env -          Entorno de OpenAI Gym sobre el cual entrenar (ej: 'SpaceInvaders-v0').\n",
    "        dqnet_class : class -    Clase que implementa la red neuronal (ej: DQNetwork). Debe heredar de `tf.keras.Model`.\n",
    "        processor : objeto  -    Objeto encargado de procesar las observaciones crudas del entorno \n",
    "                                  (por ejemplo, redimensionar y convertir a escala de grises).\n",
    "        class_to_prefix : dict - Diccionario que asocia el nombre de la clase de red a un prefijo identificador para guardar pesos y datos.\n",
    "        epsilon_start : float -  Valor inicial de epsilon (probabilidad de tomar una acción aleatoria, exploración).\n",
    "        total_episodios : int -  Número total de episodios a entrenar.\n",
    "        max_steps : int -        Número máximo de pasos por episodio.\n",
    "        batch_size : int -       Tamaño de los lotes para el entrenamiento.\n",
    "        gamma : float -          Factor de descuento para los futuros Q-valores.        \n",
    "        memory_size : int -      Tamaño máximo de la memoria de repetición (replay buffer).\n",
    "        tau : float -            Tasa de actualización suave para redes objetivo (target network).\n",
    "        start_episode : int   -  Episodio desde el cual comenzar (por ejemplo, al reanudar desde un checkpoint) (default: 0).\n",
    "        checkpoint_path : str -  Ruta al archivo de checkpoint (pesos del modelo guardados en formato `.h5`) para continuar entrenamiento (default: None).\n",
    "        memory_path : str -      Ruta al archivo de memoria de repetición guardada (`.pkl`) para restaurar la experiencia pasada (default: None).      \n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "        tf.keras.Model: Modelo entrenado.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: Si dqnet_class no está en class_to_prefix.\n",
    "        Exception: Captura errores durante el entrenamiento o la carga de archivos.\n",
    "        \n",
    "    \"\"\"    \n",
    "    # Asegurar eager execution y comportamiento NumPy para TensorFlow en este loop de entrenamiento.\n",
    "    import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "    # tf.config.run_functions_eagerly(True)\n",
    "    np_config.enable_numpy_behavior()\n",
    "    tf.keras.backend.clear_session() # Mantener esto para asegurar un estado limpio para cada modelo   \n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    tf.config.optimizer.set_jit(True)       # XLA compilation    \n",
    "    print(\"[INFO] - Eager execution deshabilitado\")\n",
    "    print(f\"[INFO] Configuración optimizada:\")\n",
    "    print(f\"  - Eager execution: {tf.executing_eagerly()}\")\n",
    "    print(f\"  - XLA habilitado: {tf.config.optimizer.get_jit()}\")\n",
    "    print(f\"  - Estado de memoria limpio\")    \n",
    "\n",
    "    # Limpiar cualquier grafo anterior\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()    \n",
    "    print(f\"[INFO] - Eager execution habilitado para {dqnet_class.__name__} (verificado: {tf.executing_eagerly()})\")    \n",
    "    \n",
    "    print(f\"[DEBUG] - Configuración optimizada:\")\n",
    "    print(f\"  - Eager execution: {tf.executing_eagerly()}\")\n",
    "    print(f\"  - XLA habilitado: {tf.config.optimizer.get_jit()}\")\n",
    "    print(f\"  - Estado de memoria limpio\")  \n",
    "    print(f\"  - State size: {state_size}\")\n",
    "    print(f\"  - Action size: {action_size}\")\n",
    "    print(f\"  - Learning rate: {learning_rate}\")\n",
    "    print(f\"  - Gamma: {gamma}\")\n",
    "    print(f\"  - TensorFlow eager: {tf.executing_eagerly()}\")\n",
    "\n",
    "    # Paso condicional de memory_size solo para DDQNetworkWithReplay\n",
    "    if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
    "        dqnet = dqnet_class(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau)       \n",
    "    elif dqnet_class in [DDQNetwork]:  # si esta clase sí necesita tau\n",
    "        dqnet = dqnet_class(state_size, action_size, learning_rate, tau=tau, gamma=gamma)\n",
    "    else:\n",
    "        dqnet = dqnet_class(state_size, action_size, learning_rate)    \n",
    "            \n",
    "    # Construir explícitamente el modelo con la forma de entrada esperada\n",
    "    # La forma de entrada al método 'call' del modelo es (batch_size, 84, 84, 4)\n",
    "    input_shape = (None,) + tuple(state_size)  # (None, 84, 84, 4)\n",
    "    dqnet.build(input_shape)\n",
    "\n",
    "    print(\"[DEBUG] - Información de la red:\\n\")  # DEBUG\n",
    "    print(dqnet.summary())    \n",
    "    # -----------------------\n",
    "    print(f\"[DEBUG] - Red creada correctamente\\n\\n\")  # DEBUG\n",
    " \n",
    "    # Cargar pesos y memoria\n",
    "    start_episode_actual = load_checkpoint_memory(dqnet, dqnet_class, class_to_prefix, checkpoint_path, memory_path, memory_size)     \n",
    "\n",
    "    scores = []\n",
    "    frame_stack = FrameStack(WINDOW_LENGTH)\n",
    "    #one_hot_matrix = tf.one_hot([action], action_size)  # Prealocado  \n",
    "    \n",
    "    # Calcular epsilon inicial basado en el start_episode_actual de inicio\n",
    "    epsilon = max(epsilon_stop, epsilon_start * (epsilon_decay ** start_episode_actual))\n",
    "    print(f\"[INFO] - Epsilon inicial ajustado a {epsilon:.3f} para episodio {start_episode_actual}\")\n",
    "\n",
    "    # CONFIGURACIÓN ANTI-MEMORY LEAK\n",
    "    TARGET_UPDATE_FREQUENCY = 50    # Menos frecuente para evitar acumulación\n",
    "    MEMORY_CLEANUP_FREQUENCY = 100  # Limpieza de memoria cada 100 steps\n",
    "    \n",
    "    # Pre-alocar arrays para evitar creación constante\n",
    "    state_shape = (1,) + tuple(state_size)    \n",
    "    gamma_tf = tf.constant(gamma, dtype=tf.float32) # Or the appropriate dtype for gamma    \n",
    "    \n",
    "    # El bucle de `tqdm` debe ir desde `start_episode_actual` ===\n",
    "    # para que el contador interno de tqdm y el `episode` en el bucle se correspondan con el episodio real.\n",
    "    for episode in trange(start_episode_actual, total_episodios, desc=\"Training\"): # Iterar usando la barra de tqdm\n",
    "        observation = env.reset()\n",
    "        frame_stack.reset()\n",
    "        state = preprocess_state(observation, processor, frame_stack)       \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        # Monitoreo de tiempo para debug\n",
    "        episode_start_time = time.time()\n",
    "        step_times = []        \n",
    "        \n",
    "        while steps < max_steps:     \n",
    "            step_start_time = time.time()\n",
    "            \n",
    "            # Selección epsilon-greedy\n",
    "            if np.random.random() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Método más robusto para obtener acción\n",
    "                state_input = tf.convert_to_tensor(np.expand_dims(state, axis=0), dtype=tf.float32)\n",
    "                q_values = dqnet(state_input, training=False)\n",
    "                # action = tf.argmax(q_values[0]).numpy()        \n",
    "                action = tf.keras.backend.get_value(tf.argmax(q_values[0]))\n",
    "                # action = tf.argmax(q_values[0]).numpy()                \n",
    "                           \n",
    "                # CRÍTICO: Limpiar referencia del tensor\n",
    "                del state_input, q_values                    \n",
    "                    \n",
    "            # Ejecutar acción\n",
    "            next_observation, reward, done, _ = env.step(action)            \n",
    "            # Procesar siguiente estado       \n",
    "            next_state = preprocess_state(next_observation, processor, frame_stack)\n",
    "            # Procesar reward (clip entre -1 y 1)\n",
    "            reward = processor.process_reward(reward)    \n",
    "            # Preparar tensores para entrenamiento            \n",
    "            rewards_tf = tf.convert_to_tensor([reward], dtype=tf.float32)\n",
    "            next_state_tf = tf.convert_to_tensor(np.expand_dims(next_state, axis=0), dtype=tf.float32)   \n",
    "            states_tf = tf.convert_to_tensor(np.expand_dims(state, axis=0), dtype=tf.float32)\n",
    "            actions_onehot = tf.convert_to_tensor([action], dtype=tf.int32)\n",
    "            dones_tf = tf.convert_to_tensor([done], dtype=tf.bool)\n",
    "\n",
    "            \n",
    "            loss = None \n",
    "\n",
    "            if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:     \n",
    "                # Almacenar en memoria\n",
    "                dqnet.store_transition(state, action, reward, next_state, done)\n",
    "                # Entrenar desde memoria si hay suficientes experiencias\n",
    "                if len(dqnet.memory) >= batch_size and steps % 4 == 0:\n",
    "                    loss = dqnet.train_from_memory(batch_size, gamma)                                     \n",
    "            elif dqnet_class == DDQNetwork:\n",
    "                start = time.time()\n",
    "                # DDQN: Usar train_step interno con lógica DDQN\n",
    "                loss = dqnet.train_step_optimized(states_tf, actions_onehot, rewards_tf, next_state_tf, dones_tf, gamma_tf)\n",
    "                print(f\"⏱ Entrenamiento minibatch: {time.time() - start:.3f} s\")  \n",
    "                # Actualizar red objetivo cada 10 pasos (para optimizar tiempos)\n",
    "                # Actualizar target network menos frecuentemente\n",
    "                if steps % TARGET_UPDATE_FREQUENCY == 0:\n",
    "                    dqnet.update_target_network()            \n",
    "            else:\n",
    "                # DQN: Calcular Q-valor objetivo manualmente\n",
    "                next_q_vals = dqnet(next_state_tf, training=False)\n",
    "                max_next_q = tf.reduce_max(next_q_vals[0])\n",
    "                # max_next_q = tf.keras.backend.get_value(tf.reduce_max(next_q_vals[0], axis=1))           \n",
    "                target_q_value = reward + gamma * max_next_q * (1.0 - float(done))      \n",
    "                target_q_tensor = tf.convert_to_tensor([target_q_value], dtype=tf.float32)                \n",
    "                # Entrenamiento usando método interno\n",
    "                loss = dqnet.train_step(states_tf, actions_onehot, target_q_tensor)\n",
    "\n",
    "            # CRÍTICO: Limpiar tensores inmediatamente después del uso\n",
    "            if loss is not None: # Solo borrar 'loss' si fue asignada\n",
    "                del loss\n",
    "            del states_tf, actions_onehot, rewards_tf, next_state_tf, dones_tf, gamma_tf                \n",
    "            \n",
    "            # CRÍTICO: Limpieza de memoria periódica\n",
    "            if steps % MEMORY_CLEANUP_FREQUENCY == 0:\n",
    "                gc.collect()\n",
    "                # Limpiar cache de TensorFlow\n",
    "                tf.keras.backend.clear_session()\n",
    "                # Reconstruir el modelo si es necesario\n",
    "                if steps > 0:  # No en el primer step\n",
    "                    dqnet._preallocate_tensors()                       \n",
    "            \n",
    "            # Actualizar estado\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Monitoreo de tiempo\n",
    "            step_time = time.time() - step_start_time\n",
    "            step_times.append(step_time)\n",
    "\n",
    "            if done:\n",
    "                print(f\"  [INFO] - Ep {episode:3d} Step {steps:3d}: *** GAME OVER *** \")                \n",
    "                break\n",
    "                \n",
    "            # Log optimizado con detección de degradación\n",
    "            if steps % 50 == 0:\n",
    "                avg_step_time = np.mean(step_times[-50:])\n",
    "                print(f\"  Ep {episode:3d} Step {steps:3d}: Avg step time: {avg_step_time:.3f}s\")\n",
    "                \n",
    "                # Alerta si el tiempo se degrada\n",
    "                if avg_step_time > 1.0:  # Si supera 1 segundo por step\n",
    "                    print(f\"  ⚠️  ADVERTENCIA: Degradación detectada ({avg_step_time:.3f}s/step)\")\n",
    "                    # Limpieza agresiva\n",
    "                    gc.collect()\n",
    "                    tf.keras.backend.clear_session()                \n",
    "                \n",
    "                \n",
    "            print(f\"- BUCLE WHILE - minibatch: {time.time() - start:.3f} s\")    \n",
    "                \n",
    "                \n",
    "        # Actualizar epsilon\n",
    "        if epsilon > epsilon_stop:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        avg_score = np.mean(scores[-10:]) if len(scores) >= 10 else np.mean(scores)\n",
    "        \n",
    "        # Estadísticas de rendimiento\n",
    "        episode_time = time.time() - episode_start_time\n",
    "        avg_step_time = episode_time / steps if steps > 0 else 0        \n",
    "        \n",
    "        print(f\"\\n📊 Episodio {episode + 1}/{total_episodios}\")\n",
    "        print(f\"   Score: {total_reward:.1f} | Steps: {steps}\")\n",
    "        print(f\"   Tiempo total: {episode_time:.1f}s | Tiempo/step: {avg_step_time:.3f}s\")\n",
    "        print(f\"   Epsilon: {epsilon:.3f} | Avg Score: {avg_score:.2f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Guardar cada 10 episodios\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            prefijo = class_to_prefix[dqnet_class]\n",
    "            save_checkpoint_memory(dqnet, dqnet_class, prefijo, episode, checkpoint_path)\n",
    "\n",
    "        # Al final del bucle de episodios\n",
    "        gc.collect()\n",
    "\n",
    "        # Detectar y alertar sobre degradación severa\n",
    "        if avg_step_time > 2.0:\n",
    "            print(f\"🚨 DEGRADACIÓN SEVERA DETECTADA: {avg_step_time:.3f}s por step\")\n",
    "            print(\"   Ejecutando limpieza completa...\")\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()        \n",
    "        \n",
    "    env.close()    \n",
    "    print(\"\\n🎯 Entrenamiento completado!\")\n",
    "    return dqnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple(trained_model, processor, episodes=3, render=False, max_steps=1000, record_video=False, video_dir=\"videos\"):\n",
    "    \"\"\"\n",
    "    Probar la red entrenada (DQN o DDQN) en el entorno SpaceInvaders-v0.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        trained_model: tf.keras.Model - Modelo entrenado (DQNetwork o DDQNetwork).\n",
    "        processor: AtariProcessor -     Procesador para preprocesar observaciones.\n",
    "        episodes: int -                 Número de episodios de prueba.\n",
    "        max_steps: int -                Máximo de pasos por episodio.\n",
    "        render: bool -                  Si se debe renderizar el entorno.\n",
    "    Retorna:\n",
    "    --------\n",
    "        float: Promedio de recompensa en los episodios de prueba.\n",
    "    \"\"\"\n",
    "    env = gym.make('SpaceInvaders-v0')\n",
    "    frame_stack = FrameStack(WINDOW_LENGTH)\n",
    "    scores = []    \n",
    "    \n",
    "    # Imprimir información de depuración\n",
    "    model_type = trained_model.__class__.__name__\n",
    "    print(f\"[DEBUG] - Probando modelo: {model_type}\")\n",
    "    print(f\"[DEBUG] - Episodios de prueba: {episodes}, Max pasos: {max_steps}, Render: {render}\\n\")    \n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        state = preprocess_state(observation, processor, frame_stack)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            # Sin exploración - solo usar la red\n",
    "            state_tensor = tf.convert_to_tensor(np.expand_dims(state, axis=0), dtype=tf.float32)\n",
    "            q_values = trained_model(state_tensor, training=False)\n",
    "            action = tf.keras.backend.get_value(tf.argmax(q_values[0]))\n",
    "            # action = tf.argmax(q_values[0]).numpy()                 \n",
    "            \n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            state = preprocess_state(next_observation, processor, frame_stack)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"  [TEST-INFO] - Ep {episode:3d} Step {steps:3d}: *** GAME OVER *** \")                      \n",
    "                break\n",
    "        \n",
    "        scores.append(total_reward)\n",
    "        print(f\"[TEST] - Episodio de prueba {episode + 1}: Score = {total_reward}, Steps = {steps}\")\n",
    "    \n",
    "    avg_test_score = np.mean(scores)\n",
    "    print(f\"\\n[TEST] - Promedio de recompensa en {episodes} episodios de prueba: {avg_test_score:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return avg_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2.1 Ejecución de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Función de utilidad para crear y comparar los modelos que se vayan creando\n",
    "def crear_modelo(model_type, \n",
    "                 state_size, \n",
    "                 action_size, \n",
    "                 total_episodios, \n",
    "                 max_steps, \n",
    "                 batch_size, \n",
    "                 gamma, \n",
    "                 epsilon_start, \n",
    "                 memory_size, \n",
    "                 tau, \n",
    "                 learning_rate=0.001,\n",
    "                 start_episode=0,\n",
    "                 checkpoint_path=checkpoint_path,\n",
    "                 memory_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Factory function para crear modelos DQN o DDQN.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model_type : str -       Tipo de modelo a entrenar. Puede ser 'DQN' o 'DDQN'.\n",
    "        state_size : tuple -     Dimensiones del estado de entrada (por ejemplo, (84, 84, 4)).\n",
    "        action_size : int-       Número de acciones posibles en el entorno.\n",
    "        total_episodios : int-   Número total de episodios de entrenamiento.\n",
    "        max_steps : int -        Número máximo de pasos por episodio.\n",
    "        batch_size : int -       Tamaño de los lotes para el entrenamiento.\n",
    "        gamma : float -          Factor de descuento para los futuros Q-valores.\n",
    "        epsilon_start : float -  Valor inicial de epsilon (probabilidad de exploración).\n",
    "        memory_size : int -      Tamaño máximo de la memoria de repetición (replay buffer).\n",
    "        tau : float -            Tasa de actualización suave para redes objetivo (target network).\n",
    "        learning_rate : float -  Tasa de aprendizaje para el optimizador (default: 0.001).\n",
    "        start_episode : int   -  Episodio desde el cual comenzar (por ejemplo, al reanudar desde un checkpoint) (default: 0).\n",
    "        checkpoint_path : str -  Ruta al archivo de checkpoint (pesos del modelo guardados en formato `.h5`) para continuar entrenamiento (default: None).\n",
    "        memory_path : str -      Ruta al archivo de memoria de repetición guardada (`.pkl`) para restaurar la experiencia pasada (default: None).\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        DQNetwork o DDQNetwork - Modelo creado según el tipo especificado\n",
    "    \"\"\"\n",
    "    # Crear el procesador Atari\n",
    "    processor = AtariProcessor()\n",
    "    \n",
    "    print(\"-\" * 60)  \n",
    "    if model_type.upper() == 'DQN':\n",
    "        print(\"Entrenando DQN simple para Space Invaders...\")\n",
    "        # Entrenar -------------------------------------\n",
    "        trained_dqn = simple_train(\n",
    "            env, DQNetwork, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,           \n",
    "            start_episode=start_episode, checkpoint_path=checkpoint_path\n",
    "        )\n",
    "    elif model_type.upper() == 'DDQN':\n",
    "        print(\"Entrenando DDQN simple para Space Invaders...\")    \n",
    "        trained_dqn = simple_train(\n",
    "            env, DDQNetwork, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,            \n",
    "            tau=tau, start_episode=start_episode, checkpoint_path=checkpoint_path\n",
    "        )             \n",
    "    elif model_type.upper() == 'DDQN_REPLAY':\n",
    "        print(\"Entrenando DDQN con Replay Memory para Space Invaders...\")\n",
    "        trained_dqn = simple_train(\n",
    "            env, DDQNetworkWithReplay, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,\n",
    "            memory_size=memory_size, tau=tau,\n",
    "            start_episode=start_episode, checkpoint_path=checkpoint_path, memory_path=memory_path\n",
    "        )\n",
    "    elif model_type.upper() == 'DUELING_DQN_REPLAY':\n",
    "        print(\"Entrenando Dueling DQN con Replay Memory para Space Invaders...\")    \n",
    "        trained_dqn = simple_train(\n",
    "            env, DuelingDQNetworkWithReplay, processor, class_to_prefix, epsilon_start, total_episodios,\n",
    "            max_steps, batch_size, gamma,\n",
    "            memory_size=memory_size, tau=tau,\n",
    "            start_episode=start_episode, checkpoint_path=checkpoint_path, memory_path=memory_path\n",
    "        )   \n",
    "    else:\n",
    "        raise ValueError(\"model_type debe ser 'DQN', 'DDQN', 'DDQN_REPLAY' o 'DUELING_DQN_REPLAY' \")\n",
    "\n",
    "    print(\"-\" * 60)       \n",
    "    return trained_dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecución de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIÓN AUXILIAR PARA PROCESAR CADA TIPO DE MODELO ---\n",
    "def _procesar_modelo_rl(\n",
    "    model_name_str: str,\n",
    "    model_class: type,\n",
    "    training_flag: bool,\n",
    "    # Parámetros generales para crear_modelo y simple_train\n",
    "    state_size: list, \n",
    "    action_size: int, \n",
    "    learning_rate: float,\n",
    "    tau: float, \n",
    "    memory_size: int, \n",
    "    checkpoint_path: str,\n",
    "    env, \n",
    "    processor, \n",
    "    epsilon_start, \n",
    "    total_episodios, \n",
    "    max_steps, \n",
    "    batch_size, \n",
    "    gamma\n",
    "):\n",
    "    \"\"\"\n",
    "    Función auxiliar que procesa la creación o carga de un modelo de RL (DQN/DDQN) \n",
    "    y ejecuta entrenamiento si es necesario.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "        model_name_str : str-        Nombre del modelo (usado para logs o checkpoints, por ejemplo: 'DQN', 'DDQN').\n",
    "        model_class : type -         Clase del modelo que hereda de `tf.keras.Model`, como `DQNetwork` o `DDQNetwork`.\n",
    "        training_flag : bool -       Si es True, se entrena el modelo desde cero o desde un checkpoint.\n",
    "                                        Si es False, se carga el modelo desde el checkpoint.\n",
    "        state_size : list -          Tamaño del estado de entrada, por ejemplo [84, 84, 4].\n",
    "        action_size : int -          Número de acciones posibles en el entorno.\n",
    "        learning_rate : float -      Tasa de aprendizaje para el optimizador.\n",
    "        tau : float -                Factor de actualización suave de pesos entre redes (target/main) usado en DDQN.\n",
    "        memory_size : int -          Tamaño de la memoria de repetición.\n",
    "        checkpoint_path : str -      Ruta para guardar o cargar los pesos del modelo entrenado (`.h5`).\n",
    "        env : gym.Env -              Entorno de Gym sobre el cual se entrena/evalúa el agente.\n",
    "        processor : objeto -         Objeto procesador que transforma la observación del entorno a la forma esperada por la red.\n",
    "        epsilon_start : float -      Valor inicial de epsilon (para exploración).\n",
    "        total_episodios : int -      Número total de episodios a entrenar.\n",
    "        max_steps : int -            Número máximo de pasos por episodio.\n",
    "        batch_size : int -           Tamaño de lote para el entrenamiento.\n",
    "        gamma : float -              Factor de descuento para los futuros Q-valores.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "        model : tf.keras.Model -       Modelo ya entrenado o cargado desde checkpoint, listo para evaluación o inferencia.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Iniciando Entrenamiento y Carga para {model_name_str} ===\")  \n",
    "    \n",
    "    model_instance = None\n",
    "    if training_flag:\n",
    "        # Llama a la función crear_modelo existente para el entrenamiento.\n",
    "        # crear_modelo ya maneja los parámetros específicos de cada tipo de red.\n",
    "        model_instance = crear_modelo(\n",
    "            model_name_str, state_size, action_size, \n",
    "            total_episodios, max_steps, batch_size, gamma, epsilon_start, # Pasando los nuevos parámetros\n",
    "            memory_size=memory_size, tau=tau, learning_rate=learning_rate,\n",
    "            start_episode=0,checkpoint_path=checkpoint_path\n",
    "        )\n",
    "    else:\n",
    "        # Si no estamos entrenando, creamos una instancia vacía para cargar pesos.\n",
    "        # Necesitamos instanciar la clase de modelo directamente.\n",
    "        if model_name_str in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY']:\n",
    "            model_instance = model_class(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau)\n",
    "        elif model_name_str == 'DDQN':\n",
    "            model_instance = model_class(state_size, action_size, learning_rate, tau=tau)\n",
    "        else: # 'DQN'\n",
    "            model_instance = model_class(state_size, action_size, learning_rate)\n",
    "        \n",
    "        # Construir el modelo si no está ya construido (necesario para cargar pesos).\n",
    "        # La forma de entrada debe incluir la dimensión del batch (None).\n",
    "        model_instance.build((None,) + tuple(state_size))\n",
    "\n",
    "    # Intentar cargar los pesos del \"mejor modelo\" guardado.\n",
    "    prefijo_modelo = class_to_prefix[model_class]\n",
    "    best_model_path = os.path.join(checkpoint_path, f'{prefijo_modelo}_best_model.h5')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            model_instance.load_weights(best_model_path)\n",
    "            print(f\"Cargado el mejor modelo {model_name_str} para pruebas desde: {best_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: No se pudieron cargar los pesos del mejor modelo {model_name_str}: {e}\")\n",
    "            # Si la carga falla y no estábamos entrenando, el modelo no está listo para probar.\n",
    "            if not training_flag:\n",
    "                model_instance = None # Indicar que este modelo no está disponible para pruebas.\n",
    "    else:\n",
    "        if training_flag and model_instance is not None:\n",
    "            print(f\"No se encontró un mejor modelo {model_name_str} guardado. Se usará la instancia recién entrenada.\")\n",
    "        else:\n",
    "            print(f\"No se encontró el mejor modelo {model_name_str} guardado y el entrenamiento está deshabilitado. Se omitirá la prueba de {model_name_str}.\")\n",
    "            model_instance = None # Indicar que este modelo no está disponible para pruebas.\n",
    "\n",
    "    if model_instance is not None:\n",
    "        print(f\"{model_name_str} listo para pruebas.\")\n",
    "    return model_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ManelGH\\AppData\\Local\\Temp\\ipykernel_8892\\1176932523.py:21: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "[INFO] - Eager execution disabled\n",
      "\n",
      "=== Iniciando Entrenamiento y Carga para DDQN ===\n",
      "------------------------------------------------------------\n",
      "Entrenando DDQN simple para Space Invaders...\n",
      "Episodio 1/3 comenzando. Memoria inicial: 518.95 MB\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 0): 533.30 MB\n",
      "⏱ Entrenamiento step: 0.524 s\n",
      "⏱ Entrenamiento 100: 0.739 s\n",
      "Step 0, Loss: 12.5985\n",
      "Memoria actual: 1708.582351MB; Pico: 1712.945912MB\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 1): 574.02 MB\n",
      "⏱ Entrenamiento step: 0.245 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 2): 579.66 MB\n",
      "⏱ Entrenamiento step: 0.302 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 3): 585.31 MB\n",
      "⏱ Entrenamiento step: 0.158 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 4): 592.52 MB\n",
      "⏱ Entrenamiento step: 0.307 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 5): 599.41 MB\n",
      "⏱ Entrenamiento step: 0.320 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 6): 606.50 MB\n",
      "⏱ Entrenamiento step: 0.300 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 7): 613.09 MB\n",
      "⏱ Entrenamiento step: 0.260 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 8): 620.69 MB\n",
      "⏱ Entrenamiento step: 0.264 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 9): 627.93 MB\n",
      "⏱ Entrenamiento step: 0.314 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 10): 634.52 MB\n",
      "⏱ Entrenamiento step: 0.294 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 11): 641.88 MB\n",
      "⏱ Entrenamiento step: 0.247 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 12): 648.66 MB\n",
      "⏱ Entrenamiento step: 0.268 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 13): 655.29 MB\n",
      "⏱ Entrenamiento step: 0.302 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 14): 662.80 MB\n",
      "⏱ Entrenamiento step: 0.302 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 15): 669.83 MB\n",
      "⏱ Entrenamiento step: 0.293 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 16): 676.30 MB\n",
      "⏱ Entrenamiento step: 0.317 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 17): 683.96 MB\n",
      "⏱ Entrenamiento step: 0.294 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 18): 690.68 MB\n",
      "⏱ Entrenamiento step: 0.225 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 19): 699.96 MB\n",
      "⏱ Entrenamiento step: 0.270 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 20): 706.72 MB\n",
      "⏱ Entrenamiento step: 0.297 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 21): 713.71 MB\n",
      "⏱ Entrenamiento step: 0.305 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 22): 720.93 MB\n",
      "⏱ Entrenamiento step: 0.174 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 23): 728.02 MB\n",
      "⏱ Entrenamiento step: 0.333 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 24): 734.88 MB\n",
      "⏱ Entrenamiento step: 0.298 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 25): 742.04 MB\n",
      "⏱ Entrenamiento step: 0.341 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 26): 748.99 MB\n",
      "⏱ Entrenamiento step: 0.283 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 27): 756.03 MB\n",
      "⏱ Entrenamiento step: 0.296 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 28): 762.27 MB\n",
      "⏱ Entrenamiento step: 0.236 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 29): 769.83 MB\n",
      "⏱ Entrenamiento step: 0.254 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 30): 776.55 MB\n",
      "⏱ Entrenamiento step: 0.159 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 31): 783.62 MB\n",
      "⏱ Entrenamiento step: 0.253 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 32): 790.67 MB\n",
      "⏱ Entrenamiento step: 0.246 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 33): 797.54 MB\n",
      "⏱ Entrenamiento step: 0.307 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 34): 804.60 MB\n",
      "⏱ Entrenamiento step: 0.317 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 35): 812.09 MB\n",
      "⏱ Entrenamiento step: 0.334 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 36): 818.75 MB\n",
      "⏱ Entrenamiento step: 0.266 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 37): 825.59 MB\n",
      "⏱ Entrenamiento step: 0.168 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 38): 833.21 MB\n",
      "⏱ Entrenamiento step: 0.323 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 39): 839.81 MB\n",
      "⏱ Entrenamiento step: 0.320 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 40): 846.58 MB\n",
      "⏱ Entrenamiento step: 0.305 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 41): 853.74 MB\n",
      "⏱ Entrenamiento step: 0.315 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 42): 860.72 MB\n",
      "⏱ Entrenamiento step: 0.311 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 43): 867.87 MB\n",
      "⏱ Entrenamiento step: 0.266 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 44): 874.50 MB\n",
      "⏱ Entrenamiento step: 0.255 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 45): 881.52 MB\n",
      "⏱ Entrenamiento step: 0.312 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 46): 888.67 MB\n",
      "⏱ Entrenamiento step: 0.241 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 47): 895.66 MB\n",
      "⏱ Entrenamiento step: 0.248 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 48): 906.77 MB\n",
      "⏱ Entrenamiento step: 0.166 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 49): 913.92 MB\n",
      "⏱ Entrenamiento step: 0.154 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 50): 921.16 MB\n",
      "⏱ Entrenamiento step: 0.252 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 51): 927.85 MB\n",
      "⏱ Entrenamiento step: 0.322 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 52): 935.00 MB\n",
      "⏱ Entrenamiento step: 0.269 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 53): 942.34 MB\n",
      "⏱ Entrenamiento step: 0.301 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 54): 949.49 MB\n",
      "⏱ Entrenamiento step: 0.267 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 55): 956.53 MB\n",
      "⏱ Entrenamiento step: 0.248 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 56): 963.18 MB\n",
      "⏱ Entrenamiento step: 0.294 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 57): 970.43 MB\n",
      "⏱ Entrenamiento step: 0.335 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 58): 977.13 MB\n",
      "⏱ Entrenamiento step: 0.312 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 59): 984.02 MB\n",
      "⏱ Entrenamiento step: 0.284 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 60): 991.19 MB\n",
      "⏱ Entrenamiento step: 0.317 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 61): 998.28 MB\n",
      "⏱ Entrenamiento step: 0.289 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 62): 1005.02 MB\n",
      "⏱ Entrenamiento step: 0.288 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 63): 1012.44 MB\n",
      "⏱ Entrenamiento step: 0.293 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 64): 1018.87 MB\n",
      "⏱ Entrenamiento step: 0.284 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 65): 1026.06 MB\n",
      "⏱ Entrenamiento step: 0.317 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 66): 1033.02 MB\n",
      "⏱ Entrenamiento step: 0.312 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 67): 1040.04 MB\n",
      "⏱ Entrenamiento step: 0.311 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 68): 1046.87 MB\n",
      "⏱ Entrenamiento step: 0.315 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 69): 1053.96 MB\n",
      "⏱ Entrenamiento step: 0.307 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 70): 1061.12 MB\n",
      "⏱ Entrenamiento step: 0.268 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 71): 1067.90 MB\n",
      "⏱ Entrenamiento step: 0.269 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 72): 1075.54 MB\n",
      "⏱ Entrenamiento step: 0.261 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 73): 1082.21 MB\n",
      "⏱ Entrenamiento step: 0.266 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 74): 1089.55 MB\n",
      "⏱ Entrenamiento step: 0.299 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 75): 1096.20 MB\n",
      "⏱ Entrenamiento step: 0.293 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 76): 1102.73 MB\n",
      "⏱ Entrenamiento step: 0.315 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 77): 1110.03 MB\n",
      "⏱ Entrenamiento step: 0.207 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria antes del minibatch (episodio 0, paso 78): 1117.52 MB\n",
      "⏱ Entrenamiento step: 0.322 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 79): 1123.86 MB\n",
      "⏱ Entrenamiento step: 0.143 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 80): 1131.29 MB\n",
      "⏱ Entrenamiento step: 0.200 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 81): 1138.00 MB\n",
      "⏱ Entrenamiento step: 0.272 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 82): 1145.34 MB\n",
      "⏱ Entrenamiento step: 0.310 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 83): 1151.67 MB\n",
      "⏱ Entrenamiento step: 0.318 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 84): 1159.08 MB\n",
      "⏱ Entrenamiento step: 0.300 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 85): 1166.03 MB\n",
      "⏱ Entrenamiento step: 0.236 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 86): 1172.66 MB\n",
      "⏱ Entrenamiento step: 0.321 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 87): 1179.73 MB\n",
      "⏱ Entrenamiento step: 0.264 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 88): 1186.78 MB\n",
      "⏱ Entrenamiento step: 0.263 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 89): 1193.68 MB\n",
      "⏱ Entrenamiento step: 0.276 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 90): 1200.87 MB\n",
      "⏱ Entrenamiento step: 0.251 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 91): 1207.99 MB\n",
      "⏱ Entrenamiento step: 0.266 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 92): 1215.55 MB\n",
      "⏱ Entrenamiento step: 0.189 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 93): 1222.67 MB\n",
      "⏱ Entrenamiento step: 0.169 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 94): 1229.72 MB\n",
      "⏱ Entrenamiento step: 0.140 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 95): 1236.11 MB\n",
      "⏱ Entrenamiento step: 0.149 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 96): 1243.42 MB\n",
      "⏱ Entrenamiento step: 0.320 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 97): 1250.27 MB\n",
      "⏱ Entrenamiento step: 0.317 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 98): 1257.19 MB\n",
      "⏱ Entrenamiento step: 0.260 s\n",
      "Uso de memoria antes del minibatch (episodio 0, paso 99): 1264.31 MB\n",
      "⏱ Entrenamiento step: 0.316 s\n",
      "Advertencia: No se puede limpiar la sesión dentro de un grafo anidado\n",
      "Episodio 1 completado en 28.47 segundos\n",
      "Memoria final: 1264.31 MB (cambio: 745.36 MB)\n",
      "Episodio 0, Recompensa total: 25.0\n",
      "⏱ Entrenamiento Episodio 0, Tiempo total: 27.778 s\n",
      "Episodio 2/3 comenzando. Memoria inicial: 2365.42 MB\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 0): 2371.04 MB\n",
      "⏱ Entrenamiento step: 0.294 s\n",
      "⏱ Entrenamiento 100: 0.564 s\n",
      "Step 0, Loss: 0.0065\n",
      "Memoria actual: 1747.838876MB; Pico: 1755.487908MB\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 1): 2430.50 MB\n",
      "⏱ Entrenamiento step: 0.324 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 2): 2448.44 MB\n",
      "⏱ Entrenamiento step: 2.135 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 3): 2454.08 MB\n",
      "⏱ Entrenamiento step: 0.217 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 4): 2459.78 MB\n",
      "⏱ Entrenamiento step: 0.337 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 5): 2466.93 MB\n",
      "⏱ Entrenamiento step: 0.327 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 6): 2472.56 MB\n",
      "⏱ Entrenamiento step: 0.302 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 7): 2478.20 MB\n",
      "⏱ Entrenamiento step: 0.303 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 8): 2491.91 MB\n",
      "⏱ Entrenamiento step: 0.329 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 9): 2604.90 MB\n",
      "⏱ Entrenamiento step: 2.333 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 10): 2623.86 MB\n",
      "⏱ Entrenamiento step: 2.263 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 11): 2667.55 MB\n",
      "⏱ Entrenamiento step: 2.201 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 12): 2673.19 MB\n",
      "⏱ Entrenamiento step: 0.258 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 13): 2678.84 MB\n",
      "⏱ Entrenamiento step: 0.249 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 14): 2684.58 MB\n",
      "⏱ Entrenamiento step: 0.154 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 15): 2690.19 MB\n",
      "⏱ Entrenamiento step: 0.250 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 16): 2695.86 MB\n",
      "⏱ Entrenamiento step: 0.284 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 17): 2701.47 MB\n",
      "⏱ Entrenamiento step: 0.285 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 18): 2802.30 MB\n",
      "⏱ Entrenamiento step: 1.720 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 19): 2826.43 MB\n",
      "⏱ Entrenamiento step: 2.303 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 20): 2832.09 MB\n",
      "⏱ Entrenamiento step: 0.244 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 21): 2865.60 MB\n",
      "⏱ Entrenamiento step: 2.208 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 22): 2871.30 MB\n",
      "⏱ Entrenamiento step: 0.258 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 23): 2876.94 MB\n",
      "⏱ Entrenamiento step: 0.250 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 24): 2922.96 MB\n",
      "⏱ Entrenamiento step: 2.392 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 25): 2928.70 MB\n",
      "⏱ Entrenamiento step: 0.361 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 26): 2934.31 MB\n",
      "⏱ Entrenamiento step: 0.318 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 27): 2939.91 MB\n",
      "⏱ Entrenamiento step: 0.238 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 28): 2945.65 MB\n",
      "⏱ Entrenamiento step: 0.280 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 29): 2951.32 MB\n",
      "⏱ Entrenamiento step: 0.246 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 30): 2956.93 MB\n",
      "⏱ Entrenamiento step: 0.303 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 31): 2962.61 MB\n",
      "⏱ Entrenamiento step: 0.259 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 32): 3075.15 MB\n",
      "⏱ Entrenamiento step: 2.590 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 33): 3080.77 MB\n",
      "⏱ Entrenamiento step: 0.164 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 34): 3086.44 MB\n",
      "⏱ Entrenamiento step: 0.280 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 35): 3092.13 MB\n",
      "⏱ Entrenamiento step: 0.307 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 36): 3097.83 MB\n",
      "⏱ Entrenamiento step: 0.298 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 37): 3170.72 MB\n",
      "⏱ Entrenamiento step: 2.545 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 38): 3191.05 MB\n",
      "⏱ Entrenamiento step: 2.690 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 39): 3211.63 MB\n",
      "⏱ Entrenamiento step: 2.689 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 40): 3231.79 MB\n",
      "⏱ Entrenamiento step: 2.746 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 41): 3252.94 MB\n",
      "⏱ Entrenamiento step: 2.667 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 42): 3258.68 MB\n",
      "⏱ Entrenamiento step: 0.147 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 43): 3291.33 MB\n",
      "⏱ Entrenamiento step: 2.453 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 44): 3296.98 MB\n",
      "⏱ Entrenamiento step: 0.314 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 45): 3330.46 MB\n",
      "⏱ Entrenamiento step: 2.649 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 46): 3349.75 MB\n",
      "⏱ Entrenamiento step: 2.767 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 47): 3355.38 MB\n",
      "⏱ Entrenamiento step: 0.152 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 48): 3361.01 MB\n",
      "⏱ Entrenamiento step: 0.261 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 49): 3366.73 MB\n",
      "⏱ Entrenamiento step: 0.158 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 50): 3372.37 MB\n",
      "⏱ Entrenamiento step: 0.450 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 51): 3377.98 MB\n",
      "⏱ Entrenamiento step: 0.170 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 52): 3462.54 MB\n",
      "⏱ Entrenamiento step: 3.013 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 53): 3468.29 MB\n",
      "⏱ Entrenamiento step: 0.223 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 54): 3473.89 MB\n",
      "⏱ Entrenamiento step: 0.229 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 55): 3479.57 MB\n",
      "⏱ Entrenamiento step: 0.282 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 56): 3486.23 MB\n",
      "⏱ Entrenamiento step: 0.299 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria antes del minibatch (episodio 1, paso 57): 3491.87 MB\n",
      "⏱ Entrenamiento step: 0.253 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 58): 3497.48 MB\n",
      "⏱ Entrenamiento step: 0.313 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 59): 3595.76 MB\n",
      "⏱ Entrenamiento step: 3.158 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 60): 3615.22 MB\n",
      "⏱ Entrenamiento step: 3.106 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 61): 3620.83 MB\n",
      "⏱ Entrenamiento step: 0.152 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 62): 3626.57 MB\n",
      "⏱ Entrenamiento step: 0.231 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 63): 3632.21 MB\n",
      "⏱ Entrenamiento step: 0.254 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 64): 3637.85 MB\n",
      "⏱ Entrenamiento step: 0.150 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 65): 3643.45 MB\n",
      "⏱ Entrenamiento step: 0.244 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 66): 3649.20 MB\n",
      "⏱ Entrenamiento step: 0.155 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 67): 3654.87 MB\n",
      "⏱ Entrenamiento step: 0.170 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 68): 3765.73 MB\n",
      "⏱ Entrenamiento step: 3.285 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 69): 3771.46 MB\n",
      "⏱ Entrenamiento step: 0.279 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 70): 3777.14 MB\n",
      "⏱ Entrenamiento step: 0.151 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 71): 3782.72 MB\n",
      "⏱ Entrenamiento step: 0.268 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 72): 3788.43 MB\n",
      "⏱ Entrenamiento step: 0.316 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 73): 3794.10 MB\n",
      "⏱ Entrenamiento step: 0.279 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 74): 3799.72 MB\n",
      "⏱ Entrenamiento step: 0.320 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 75): 3805.38 MB\n",
      "⏱ Entrenamiento step: 0.423 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 76): 3916.46 MB\n",
      "⏱ Entrenamiento step: 3.464 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 77): 3936.33 MB\n",
      "⏱ Entrenamiento step: 3.234 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 78): 3956.54 MB\n",
      "⏱ Entrenamiento step: 3.405 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 79): 3962.25 MB\n",
      "⏱ Entrenamiento step: 0.323 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 80): 3967.91 MB\n",
      "⏱ Entrenamiento step: 0.320 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 81): 4014.33 MB\n",
      "⏱ Entrenamiento step: 3.462 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 82): 4019.92 MB\n",
      "⏱ Entrenamiento step: 0.296 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 83): 4026.85 MB\n",
      "⏱ Entrenamiento step: 0.227 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 84): 4032.54 MB\n",
      "⏱ Entrenamiento step: 0.238 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 85): 4038.21 MB\n",
      "⏱ Entrenamiento step: 0.319 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 86): 4043.85 MB\n",
      "⏱ Entrenamiento step: 0.316 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 87): 4049.60 MB\n",
      "⏱ Entrenamiento step: 0.318 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 88): 4146.49 MB\n",
      "⏱ Entrenamiento step: 3.576 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 89): 4152.10 MB\n",
      "⏱ Entrenamiento step: 0.257 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 90): 4157.82 MB\n",
      "⏱ Entrenamiento step: 0.244 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 91): 4203.82 MB\n",
      "⏱ Entrenamiento step: 3.464 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 92): 4209.44 MB\n",
      "⏱ Entrenamiento step: 0.232 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 93): 4242.31 MB\n",
      "⏱ Entrenamiento step: 3.570 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 94): 4247.98 MB\n",
      "⏱ Entrenamiento step: 0.304 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 95): 4253.66 MB\n",
      "⏱ Entrenamiento step: 0.169 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 96): 4300.91 MB\n",
      "⏱ Entrenamiento step: 3.601 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 97): 4306.61 MB\n",
      "⏱ Entrenamiento step: 0.255 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 98): 4312.31 MB\n",
      "⏱ Entrenamiento step: 0.144 s\n",
      "Uso de memoria antes del minibatch (episodio 1, paso 99): 4358.74 MB\n",
      "⏱ Entrenamiento step: 3.392 s\n",
      "Advertencia: No se puede limpiar la sesión dentro de un grafo anidado\n",
      "Episodio 2 completado en 105.95 segundos\n",
      "Memoria final: 4358.74 MB (cambio: 1993.32 MB)\n",
      "Limpieza de memoria realizada (sin clear_session)\n",
      "Episodio 1, Recompensa total: 25.0\n",
      "⏱ Entrenamiento Episodio 1, Tiempo total: 121.782 s\n",
      "Episodio 3/3 comenzando. Memoria inicial: 4386.43 MB\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 0): 4392.55 MB\n",
      "⏱ Entrenamiento step: 3.894 s\n",
      "⏱ Entrenamiento 100: 4.245 s\n",
      "Step 0, Loss: 0.0061\n",
      "Memoria actual: 1791.795298MB; Pico: 1796.608224MB\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 1): 4427.06 MB\n",
      "⏱ Entrenamiento step: 0.176 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 2): 4434.67 MB\n",
      "⏱ Entrenamiento step: 0.316 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 3): 4466.54 MB\n",
      "⏱ Entrenamiento step: 3.837 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 4): 4487.33 MB\n",
      "⏱ Entrenamiento step: 3.871 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 5): 4507.45 MB\n",
      "⏱ Entrenamiento step: 3.916 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 6): 4529.80 MB\n",
      "⏱ Entrenamiento step: 3.947 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 7): 4535.47 MB\n",
      "⏱ Entrenamiento step: 0.320 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 8): 4567.88 MB\n",
      "⏱ Entrenamiento step: 3.728 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 9): 4587.57 MB\n",
      "⏱ Entrenamiento step: 3.885 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 10): 4593.25 MB\n",
      "⏱ Entrenamiento step: 0.158 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 11): 4626.10 MB\n",
      "⏱ Entrenamiento step: 3.751 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 12): 4631.74 MB\n",
      "⏱ Entrenamiento step: 0.269 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 13): 4637.36 MB\n",
      "⏱ Entrenamiento step: 0.238 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 14): 4683.77 MB\n",
      "⏱ Entrenamiento step: 3.975 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 15): 4703.13 MB\n",
      "⏱ Entrenamiento step: 4.087 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 16): 4722.99 MB\n",
      "⏱ Entrenamiento step: 4.114 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 17): 4742.86 MB\n",
      "⏱ Entrenamiento step: 4.087 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 18): 4763.39 MB\n",
      "⏱ Entrenamiento step: 3.762 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 19): 4785.05 MB\n",
      "⏱ Entrenamiento step: 0.365 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 20): 4817.67 MB\n",
      "⏱ Entrenamiento step: 4.302 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 21): 4837.85 MB\n",
      "⏱ Entrenamiento step: 3.944 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 22): 4843.46 MB\n",
      "⏱ Entrenamiento step: 0.142 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 23): 4876.22 MB\n",
      "⏱ Entrenamiento step: 4.191 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 24): 4896.47 MB\n",
      "⏱ Entrenamiento step: 4.244 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 25): 4902.13 MB\n",
      "⏱ Entrenamiento step: 0.468 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 26): 4907.77 MB\n",
      "⏱ Entrenamiento step: 0.282 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 27): 4953.78 MB\n",
      "⏱ Entrenamiento step: 4.135 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 28): 4973.98 MB\n",
      "⏱ Entrenamiento step: 4.267 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 29): 4993.76 MB\n",
      "⏱ Entrenamiento step: 4.289 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 30): 5013.91 MB\n",
      "⏱ Entrenamiento step: 4.267 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 31): 5033.89 MB\n",
      "⏱ Entrenamiento step: 4.172 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 32): 5039.64 MB\n",
      "⏱ Entrenamiento step: 0.269 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 33): 5072.69 MB\n",
      "⏱ Entrenamiento step: 4.259 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 34): 5078.39 MB\n",
      "⏱ Entrenamiento step: 0.154 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 35): 5084.09 MB\n",
      "⏱ Entrenamiento step: 0.178 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso de memoria antes del minibatch (episodio 2, paso 36): 5129.41 MB\n",
      "⏱ Entrenamiento step: 4.232 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 37): 5149.39 MB\n",
      "⏱ Entrenamiento step: 4.358 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 38): 5169.37 MB\n",
      "⏱ Entrenamiento step: 4.483 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 39): 5174.98 MB\n",
      "⏱ Entrenamiento step: 0.143 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 40): 5207.62 MB\n",
      "⏱ Entrenamiento step: 4.570 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 41): 5227.83 MB\n",
      "⏱ Entrenamiento step: 4.264 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 42): 5248.36 MB\n",
      "⏱ Entrenamiento step: 4.515 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 43): 5254.70 MB\n",
      "⏱ Entrenamiento step: 0.164 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 44): 5286.68 MB\n",
      "⏱ Entrenamiento step: 4.312 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 45): 5306.70 MB\n",
      "⏱ Entrenamiento step: 4.650 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 46): 5326.25 MB\n",
      "⏱ Entrenamiento step: 4.525 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 47): 5345.67 MB\n",
      "⏱ Entrenamiento step: 4.503 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 48): 5365.87 MB\n",
      "⏱ Entrenamiento step: 4.633 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 49): 5371.50 MB\n",
      "⏱ Entrenamiento step: 0.205 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 50): 5377.16 MB\n",
      "⏱ Entrenamiento step: 0.268 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 51): 5423.15 MB\n",
      "⏱ Entrenamiento step: 4.291 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 52): 5428.86 MB\n",
      "⏱ Entrenamiento step: 0.159 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 53): 5434.46 MB\n",
      "⏱ Entrenamiento step: 0.174 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 54): 5480.55 MB\n",
      "⏱ Entrenamiento step: 4.528 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 55): 5502.38 MB\n",
      "⏱ Entrenamiento step: 4.599 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 56): 5520.97 MB\n",
      "⏱ Entrenamiento step: 4.736 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 57): 5526.61 MB\n",
      "⏱ Entrenamiento step: 0.332 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 58): 5559.21 MB\n",
      "⏱ Entrenamiento step: 4.876 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 59): 5564.91 MB\n",
      "⏱ Entrenamiento step: 0.204 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 60): 5597.54 MB\n",
      "⏱ Entrenamiento step: 4.682 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 61): 5617.29 MB\n",
      "⏱ Entrenamiento step: 4.750 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 62): 5637.55 MB\n",
      "⏱ Entrenamiento step: 4.924 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 63): 5657.14 MB\n",
      "⏱ Entrenamiento step: 4.966 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 64): 5662.78 MB\n",
      "⏱ Entrenamiento step: 0.343 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 65): 5695.54 MB\n",
      "⏱ Entrenamiento step: 4.838 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 66): 5715.62 MB\n",
      "⏱ Entrenamiento step: 4.810 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 67): 5721.80 MB\n",
      "⏱ Entrenamiento step: 0.264 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 68): 5727.50 MB\n",
      "⏱ Entrenamiento step: 0.304 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 69): 5773.26 MB\n",
      "⏱ Entrenamiento step: 4.919 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 70): 5794.97 MB\n",
      "⏱ Entrenamiento step: 4.745 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 71): 5813.29 MB\n",
      "⏱ Entrenamiento step: 5.105 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 72): 5834.38 MB\n",
      "⏱ Entrenamiento step: 5.162 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 73): 5853.61 MB\n",
      "⏱ Entrenamiento step: 4.672 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 74): 5859.19 MB\n",
      "⏱ Entrenamiento step: 0.303 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 75): 5892.47 MB\n",
      "⏱ Entrenamiento step: 5.262 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 76): 5912.25 MB\n",
      "⏱ Entrenamiento step: 5.187 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 77): 5917.87 MB\n",
      "⏱ Entrenamiento step: 0.270 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 78): 5951.18 MB\n",
      "⏱ Entrenamiento step: 5.246 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 79): 5971.19 MB\n",
      "⏱ Entrenamiento step: 5.099 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 80): 5991.73 MB\n",
      "⏱ Entrenamiento step: 5.049 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 81): 6012.95 MB\n",
      "⏱ Entrenamiento step: 5.317 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 82): 6033.53 MB\n",
      "⏱ Entrenamiento step: 5.250 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 83): 6053.27 MB\n",
      "⏱ Entrenamiento step: 5.188 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 84): 6058.99 MB\n",
      "⏱ Entrenamiento step: 0.296 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 85): 6092.61 MB\n",
      "⏱ Entrenamiento step: 4.680 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 86): 6112.34 MB\n",
      "⏱ Entrenamiento step: 5.269 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 87): 6132.42 MB\n",
      "⏱ Entrenamiento step: 5.468 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 88): 6152.12 MB\n",
      "⏱ Entrenamiento step: 4.991 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 89): 6157.87 MB\n",
      "⏱ Entrenamiento step: 0.318 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 90): 6191.46 MB\n",
      "⏱ Entrenamiento step: 5.484 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 91): 6211.46 MB\n",
      "⏱ Entrenamiento step: 5.271 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 92): 6231.51 MB\n",
      "⏱ Entrenamiento step: 5.329 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 93): 6251.61 MB\n",
      "⏱ Entrenamiento step: 5.442 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 94): 6272.24 MB\n",
      "⏱ Entrenamiento step: 5.538 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 95): 6291.35 MB\n",
      "⏱ Entrenamiento step: 5.457 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 96): 6311.79 MB\n",
      "⏱ Entrenamiento step: 5.355 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 97): 6332.24 MB\n",
      "⏱ Entrenamiento step: 5.535 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 98): 6337.88 MB\n",
      "⏱ Entrenamiento step: 0.154 s\n",
      "Uso de memoria antes del minibatch (episodio 2, paso 99): 6370.53 MB\n",
      "⏱ Entrenamiento step: 5.436 s\n",
      "Advertencia: No se puede limpiar la sesión dentro de un grafo anidado\n",
      "Episodio 3 completado en 345.51 segundos\n",
      "Memoria final: 6370.53 MB (cambio: 1984.09 MB)\n",
      "Limpieza de memoria realizada (sin clear_session)\n",
      "Episodio 2, Recompensa total: 0.0\n",
      "⏱ Entrenamiento Episodio 2, Tiempo total: 360.848 s\n",
      "------------------------------------------------------------\n",
      "No se encontró un mejor modelo DDQN guardado. Se usará la instancia recién entrenada.\n",
      "DDQN listo para pruebas.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress deprecated warnings (optional, for cleaner output)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Crear carpeta para diagnósticos si no existe\n",
    "os.makedirs(\"diagnosticos\", exist_ok=True)\n",
    "\n",
    "# Métricas para monitorear el rendimiento\n",
    "episode_times = []\n",
    "memory_usage = []\n",
    "\n",
    "# Configuración TensorFlow para prevenir fugas\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# Main block\n",
    "if __name__ == \"__main__\":\n",
    "    graph = tf.Graph()\n",
    "    with tf.compat.v1.Session(graph=graph) as sess:\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "        tf.config.run_functions_eagerly(True)\n",
    "        tf.config.optimizer.set_jit(False)\n",
    "        print(\"[INFO] - Eager execution disabled\")\n",
    "        \n",
    "        model_instance = _procesar_modelo_rl(\n",
    "            model_name_str='DDQN',\n",
    "            model_class=DDQNetwork,\n",
    "            training_flag=True,\n",
    "            state_size=(84, 84, 3),\n",
    "            action_size=env.action_space.n,\n",
    "            learning_rate=0.00025,\n",
    "            tau=tau,\n",
    "            memory_size=memory_size,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            env=env,\n",
    "            processor=processor,\n",
    "            epsilon_start=epsilon_start,\n",
    "            total_episodios=3,\n",
    "            max_steps=100,\n",
    "            batch_size=batch_size,\n",
    "            gamma=gamma\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Do not use tf.reset_default_graph() to clear nested graphs. If you need a cleared graph, exit the nesting and create a new graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSession(graph\u001b[38;5;241m=\u001b[39mgraph) \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[0;32m      3\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mset_session(sess)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrun_functions_eagerly(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m     tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mset_jit(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Desactivar XLA para la prueba\u001b[39;00m\n",
      "File \u001b[1;32mD:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\keras\\backend.py:279\u001b[0m, in \u001b[0;36mclear_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _GRAPH\n\u001b[0;32m    278\u001b[0m _GRAPH\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_default_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m reset_uids()\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _SESSION\u001b[38;5;241m.\u001b[39msession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\90-Tools\\anaconda3\\envs\\mghMiar08\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6279\u001b[0m, in \u001b[0;36mreset_default_graph\u001b[1;34m()\u001b[0m\n\u001b[0;32m   6257\u001b[0m \u001b[38;5;124;03m\"\"\"Clears the default graph stack and resets the global default graph.\u001b[39;00m\n\u001b[0;32m   6258\u001b[0m \n\u001b[0;32m   6259\u001b[0m \u001b[38;5;124;03mNOTE: The default graph is a property of the current thread. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6276\u001b[0m \u001b[38;5;124;03m  AssertionError: If this function is called within a nested graph.\u001b[39;00m\n\u001b[0;32m   6277\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _default_graph_stack\u001b[38;5;241m.\u001b[39mis_cleared():\n\u001b[1;32m-> 6279\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo not use tf.reset_default_graph() to clear \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6280\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested graphs. If you need a cleared graph, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6281\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit the nesting and create a new graph.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6282\u001b[0m _default_graph_stack\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[1;31mAssertionError\u001b[0m: Do not use tf.reset_default_graph() to clear nested graphs. If you need a cleared graph, exit the nesting and create a new graph."
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    with tf.compat.v1.Session(graph=graph) as sess:\n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.config.run_functions_eagerly(False)\n",
    "        tf.config.optimizer.set_jit(True)  # Desactivar XLA para la prueba\n",
    "\n",
    "        model_instance = _procesar_modelo_rl(\n",
    "            model_name_str='DDQN',\n",
    "            model_class=DDQNetwork,\n",
    "            training_flag=True,\n",
    "            state_size=state_size,\n",
    "            action_size=action_size,\n",
    "            learning_rate=learning_rate,\n",
    "            tau=tau,\n",
    "            memory_size=memory_size,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            env=env,\n",
    "            processor=processor,\n",
    "            epsilon_start=epsilon_start,\n",
    "            total_episodios=2,  # Reducir para prueba\n",
    "            max_steps=100,      # Reducir para prueba\n",
    "            batch_size=batch_size,\n",
    "            gamma=gamma\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- Bloque de Ejecución Principal ---\n",
    "if __name__ == \"__main__\":   \n",
    "    # Crear un grafo explícito\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            with tf.compat.v1.Session(graph=graph) as sess:\n",
    "                tf.compat.v1.keras.backend.set_session(sess)\n",
    "    \n",
    "                import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "                # Limpiar cualquier grafo anterior\n",
    "                tf.keras.backend.clear_session()\n",
    "                # Enable NumPy behavior and force eager execution at the start\n",
    "                np_config.enable_numpy_behavior()\n",
    "                tf.config.run_functions_eagerly(False)\n",
    "                tf.config.optimizer.set_jit(True)       # XLA compilation    \n",
    "                print(\"[INFO] - Eager execution deshabilitado\")\n",
    "\n",
    "                # Control global de si se entrena o solo se carga\n",
    "                training_global = True \n",
    "                # Control de renderizado durante el entrenamiento (no afecta la grabación de video final)\n",
    "                episode_render = False   \n",
    "    \n",
    "                # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "\n",
    "                # Diccionario para guardar los *mejores modelos cargados/entrenados* de cada tipo\n",
    "                trained_models = {}\n",
    "                # Lista de tuplas (nombre_modelo, clase_modelo, flag_entrenamiento_especifico)\n",
    "                modelos_a_procesar = [\n",
    "                    ('DQN', DQNetwork, False), # NO Entrenar DQN\n",
    "                    ('DDQN', DDQNetwork, True), # No entrenar DDQN, solo cargar si existe\n",
    "                    ('DDQN_REPLAY', DDQNetworkWithReplay, False), # No entrenar DDQN_REPLAY\n",
    "                    ('DUELING_DQN_REPLAY', DuelingDQNetworkWithReplay, False) # Entrenar DuelingDQN_REPLAY\n",
    "                ]    \n",
    "\n",
    "                for model_name, model_class, training_specific_flag in modelos_a_procesar:\n",
    "                    tf.keras.backend.clear_session()  # Limpiar antes de cada modelo\n",
    "                    # La bandera de entrenamiento final es la global AND la específica del modelo\n",
    "                    entrenarSN = training_global and training_specific_flag   \n",
    "                    current_model_instance = _procesar_modelo_rl(\n",
    "                        model_name_str=model_name,\n",
    "                        model_class=model_class,\n",
    "                        training_flag=entrenarSN,\n",
    "                        state_size=state_size,\n",
    "                        action_size=action_size,\n",
    "                        learning_rate=learning_rate,\n",
    "                        tau=tau,\n",
    "                        memory_size=memory_size,\n",
    "                        checkpoint_path=checkpoint_path,\n",
    "                        env=env,\n",
    "                        processor=processor,\n",
    "                        epsilon_start=epsilon_start,\n",
    "                        total_episodios=total_episodios,\n",
    "                        max_steps=max_steps,\n",
    "                        batch_size=batch_size,\n",
    "                        gamma=gamma\n",
    "                    )\n",
    "                    if current_model_instance is not None:\n",
    "                        trained_models[model_name] = current_model_instance\n",
    "                    else:\n",
    "                        print(f\"[{model_name}] Modelo no disponible para pruebas.\")                \n",
    "                \n",
    "    # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # ------------ PRUEBA FINAL Y GRABACIÓN DE VIDEO --------------------------------------\n",
    "    print(\"\\n====== Resumen de Resultados y Prueba Final ======\")   \n",
    "\n",
    "    best_overall_score = -np.inf\n",
    "    best_overall_model_type = None\n",
    "    record_video = False\n",
    "    test_results = {}\n",
    "\n",
    "    for model_name, model_instance in trained_models.items():\n",
    "        print(f\"\\n=== Probando {model_name} entrenada ===\")\n",
    "        score = test_simple(\n",
    "            trained_model=model_instance, \n",
    "            processor=processor, \n",
    "            episodes=5,              # Puedes aumentar esto para una prueba más robusta\n",
    "            render=episode_render,   # No renderizar en la ventana, solo grabar\n",
    "            record_video=record_video, \n",
    "            video_dir=f'videos/{model_name}_final_game' # Directorio de video específico para cada modelo\n",
    "        )\n",
    "        test_results[model_name] = score\n",
    "        \n",
    "        # Actualizar el mejor modelo global\n",
    "        if score > best_overall_score:\n",
    "            best_overall_score = score\n",
    "            best_overall_model_type = model_name\n",
    "\n",
    "    print(\"\\n====== Resultados Promedio de Pruebas ======\")\n",
    "    for model_name, score in test_results.items():\n",
    "        print(f\"{model_name} Promedio: {score:.2f}\")\n",
    "\n",
    "    if best_overall_model_type:\n",
    "        print(f\"\\nEl MEJOR modelo general es: {best_overall_model_type} con un score promedio de {best_overall_score:.2f}\")\n",
    "        print(f\"Puedes encontrar el video de su ejecución en el directorio 'videos/{best_overall_model_type}_final_game'\")\n",
    "    else:\n",
    "        print(\"No se entrenó ningún modelo o no se pudo determinar el mejor para la prueba final.\")\n",
    "    \"\"\"\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (mghMiar08)",
   "language": "python",
   "name": "mghmiar08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
