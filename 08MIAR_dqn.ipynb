{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManelGH1167/prueba/blob/main/08MIAR_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSVPAihG4U1j"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "* Alumno 1: Benali, Abdelilah\n",
        "* Alumno 2: Cuesta Cifuentes, Jair\n",
        "* Alumno 3: González Huete, Manel\n",
        "* Alumno 4: Manzanas Mogrovejo, Francisco\n",
        "* Alumno 5: Pascual, Guadalupe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWWcufoC7S2B"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1svUw2WiJAUy"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda update --all\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouO30DIAKL3"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cw5W3OopAFKN"
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/VIU/08_AR_MIAR/sesiones_practicas/sesion_practica_1\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK5sY_ybAFt8"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "3lN7KLe05NSa",
        "outputId": "47c41c84-3bfd-425f-9b8d-6d6aa525afdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3572505322.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Colab: mounting Google drive on \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Create drive_root if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2zVSAPW43MH"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmVd8L9XBt26",
        "outputId": "bfacbba7-1ac1-4fe8-ff9e-28502fcb8931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym==0.17.3 in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.23.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "Collecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-y2de4m7e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-y2de4m7e\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (1.23.5)\n",
            "Requirement already satisfied: keras-rl2==1.0.5 in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.73.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2==1.0.5) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.8 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.8\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "IN_LOCAL = True\n",
        "\n",
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.8\n",
        "if not IN_LOCAL:\n",
        "  %pip install numpy==1.23.5\n",
        "  %pip install gym==0.17\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0\n",
        "  %pip install matplotlib==3.4.3\n",
        "  %pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihTI9TOD43ML"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIAR9zQv43MO"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas\n",
        "\n",
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "K4o4-N4T43MO"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import gc       # Para garbage collection\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import re       # Para expresiones regulares en carga de checkpoints\n",
        "import gym      # Para el entorno de Atari\n",
        "import cv2     # Para preprocesamiento de imágenes si se usa AtariProcessor\n",
        "import warnings\n",
        "import time\n",
        "import psutil\n",
        "import tracemalloc\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.agents.dqn import DQNAgent, AbstractDQNAgent\n",
        "\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from tensorflow.keras.layers import Lambda, Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "from collections import deque\n",
        "from tqdm import trange     # Necesaria para la barra de progreso en simple_train\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necesario para la grabación de video\n",
        "try:\n",
        "    import gym.wrappers\n",
        "except ImportError:\n",
        "    print(\"WARNING: gym.wrappers no está disponible. La grabación de video no funcionará.\")\n",
        "    gym.wrappers = None # Asegurar que no dé error si no se encuentra\n"
      ],
      "metadata": {
        "id": "puzS2kTzc1Fd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar TensorFlow para CPU (14 cores)\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
        "\n",
        "# Ajuste recomendado (seguro y eficiente)\n",
        "tf.config.threading.set_intra_op_parallelism_threads(8)   # dentro de cada operación\n",
        "tf.config.threading.set_inter_op_parallelism_threads(4)   # entre operaciones\n",
        "# Limpiar cualquier sesión previa\n",
        "tf.keras.backend.clear_session()\n",
        "print(\"Configurando TensorFlow para evitar fugas de memoria...\")\n",
        "\n",
        "# Forzar modo eager (crucial)\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8iRCStcc90p",
        "outputId": "96045a07-febd-4dce-e5c8-bbec04475b33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configurando TensorFlow para evitar fugas de memoria...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Crear el entorno\n",
        "Nuestro entorno es el juego Space Invaders, de Atari"
      ],
      "metadata": {
        "id": "faNbnMuOdNDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el entorno\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "7WFE0sqPdLsy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"El tamaño de nuestro 'frame' es: \", env.observation_space)\n",
        "print(\"El número de acciones posibles es : \", nb_actions)\n",
        "print(\"Las acciones posibles son : \",env.env.get_action_meanings())\n",
        "\n",
        "# Here we create an hot encoded version of our actions\n",
        "# possible_actions = [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]...]\n",
        "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "print(\"\\nOHE de las acciones posibles: \\n\", possible_actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcjoyH3idqh4",
        "outputId": "85af5e47-6b37-472b-ddbe-ce71bea7eb33"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El tamaño de nuestro 'frame' es:  Box(0, 255, (210, 160, 3), uint8)\n",
            "El número de acciones posibles es :  6\n",
            "Las acciones posibles son :  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "\n",
            "OHE de las acciones posibles: \n",
            " [[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definición Hiperparámetros"
      ],
      "metadata": {
        "id": "tgw7iHVRduGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### HIPERPARÁMETROS DEL MODELO\n",
        "\n",
        "### HIPERPARÁMETROS DE PREPROCESAMIENTO\n",
        "USE_FRAMESTACK = True  # Cambiar a True si quieres detección de movimiento\n",
        "WINDOW_LENGTH = 3 if USE_FRAMESTACK else 1   # Número de fotogramas apilados\n",
        "\n",
        "# Definir shape consistente\n",
        "if USE_FRAMESTACK:\n",
        "    state_shape = (84, 84, WINDOW_LENGTH)  # (84, 84, x)\n",
        "else:\n",
        "    state_shape = (84, 84, 1)  # (84, 84, 1) - escala de grises simple\n",
        "\n",
        "INPUT_SHAPE = (84, 84)\n",
        "state_size = [84, 84, 3]          # Nuestra entrada es una pila de 4 fotogramas, por lo tanto 110x84x4 (ancho, alto, canales)\n",
        "action_size = env.action_space.n  # 6 acciones posibles\n",
        "learning_rate =  0.00025          # Alfa (también conocido como tasa de aprendizaje)\n",
        "\n",
        "### HIPERPARÁMETROS DE ENTRENAMIENTO\n",
        "# total_episodios = 10    #TEST        # Episodios totales para el entrenamiento\n",
        "# max_steps = 10000       #TEST        # Máximo de pasos posibles por episodio\n",
        "total_episodios = 100          # Episodios totales para el entrenamiento\n",
        "max_steps = 3000               # Máximo de pasos posibles por episodio\n",
        "batch_size = 32                # Tamaño del lote (batch)\n",
        "\n",
        "# Parámetros de exploración para la estrategia epsilon-greedy\n",
        "epsilon_start = 1.0            # Probabilidad de exploración al inicio\n",
        "epsilon_stop = 0.01            # Probabilidad mínima de exploración\n",
        "decay_rate = 0.00001           # Tasa de decaimiento exponencial para la probabilidad de exploración\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "# Hiperparámetros del aprendizaje Q\n",
        "gamma = 0.95                   # Tasa de descuento\n",
        "tau = 0.001\n",
        "checkpoint_path=\"checkpoints\"\n",
        "\n",
        "### HIPERPARÁMETROS DE MEMORIA\n",
        "pretrain_length = batch_size   # Número de experiencias almacenadas en la memoria al inicializar por primera vez\n",
        "memory_size = 5000             # Número de experiencias que la memoria puede guardar\n",
        "\n",
        "### CAMBIA ESTO A FALSE SI SOLO QUIERES VER AL AGENTE ENTRENADO\n",
        "training = False\n",
        "\n",
        "## CAMBIA ESTO A TRUE SI QUIERES RENDERIZAR EL ENTORNO\n",
        "episode_render = False"
      ],
      "metadata": {
        "id": "TshMrqTjdxja"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clase \"processor\" para Atari\n",
        "\n",
        "Ahora definimos un \"processor\" para las pantallas de entrada del juego, en el que recortamos el tamaño de la imagen (matriz de 210 x 160 píxeles) y la convertimos En una matriz bidimensional de 80 x 80 píxeles). También convertimos las imágenes de RGB a escala de grises normal, ya que no necesitamos usar los colores. Con este trabajo buscamos acelerar nuestro algoritmo, eliminando la información innecesaria y reduciendo la carga de la GPU."
      ],
      "metadata": {
        "id": "cjFRyr4Ld1Hp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "06wZVH5c43MP"
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    \"\"\"\n",
        "    Procesador para preprocesar observaciones del entorno Atari (e.g., SpaceInvaders-v0).\n",
        "\n",
        "    Hereda de rl.core.Processor y proporciona métodos para convertir observaciones RGB en\n",
        "    imágenes en escala de grises, redimensionarlas y normalizarlas, así como para limitar\n",
        "    las recompensas.\n",
        "\n",
        "    MÉTODOS:\n",
        "    --------\n",
        "        process_observation(observation): Convierte una observación RGB a escala de grises\n",
        "                                         y la redimensiona.\n",
        "        process_state_batch(batch): Normaliza un lote de estados dividiendo por 255.\n",
        "        process_reward(reward): Limita las recompensas a un rango [-1, 1].\n",
        "    \"\"\"\n",
        "    def process_observation(self, observation):\n",
        "        \"\"\"\n",
        "        Preprocesa una observación convirtiéndola a escala de grises y redimensionándola.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "            observation (np.ndarray): Observación cruda del entorno con forma (height, width, channels).\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "            np.ndarray: Imagen en escala de grises redimensionada a INPUT_SHAPE (84, 84) en formato uint8.\n",
        "\n",
        "        Raises:\n",
        "            AssertionError: Si la observación no tiene 3 dimensiones o la forma procesada no coincide con INPUT_SHAPE.\n",
        "        \"\"\"\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        # Crop the screen (remove the part below the player)\n",
        "        # [Up: Down, Left: right]\n",
        "        cropped_img = observation[18:-12, 4:-12]\n",
        "        img = Image.fromarray(cropped_img)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Normaliza un lote de estados dividiendo los valores por 255.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
        "        \"\"\"\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        \"\"\"\n",
        "        Normaliza un lote de estados dividiendo los valores por 255.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "            batch (np.ndarray): Lote de estados con valores en [0, 255].\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "            np.ndarray: Lote normalizado con valores en [0, 1] en formato float32.\n",
        "        \"\"\"\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "    def process_step(self, observation, reward, done, info):\n",
        "        \"\"\"\n",
        "        Procesa un paso completo del entorno.\n",
        "        \"\"\"\n",
        "        processed_observation = self.process_observation(observation)\n",
        "        processed_reward = self.process_reward(reward)\n",
        "        return processed_observation, processed_reward, done, info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Revisar el entorno de juego"
      ],
      "metadata": {
        "id": "ptwluQRXedZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VwBXOBk43MP",
        "outputId": "88dbba58-92a4-4d75-cf4c-799a04c36cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de acciones disponibles: 6\n"
          ]
        }
      ],
      "source": [
        "print(\"Numero de acciones disponibles: \" + str(nb_actions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT6osc3H43MP",
        "outputId": "cbe27690-c40d-49b1-d420-80b7d045ce26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formato de las observaciones:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "print(\"Formato de las observaciones:\")\n",
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "for i in range(22):\n",
        "  if i > 20:\n",
        "    plt.imshow(observation)\n",
        "    plt.show()\n",
        "\n",
        "  observation, reward, done, info = env.step(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "7-BoOu_eeiAE",
        "outputId": "5b52561e-70d0-47c1-f55e-c882dda20b66"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las entradas preprocesadas en escala de grises y comparar originales y preprocesados.\n",
        "processor = AtariProcessor()\n",
        "obs_preprocessed = processor.process_observation(observation).reshape(INPUT_SHAPE)\n",
        "# Seleccionamos el primer frame y lo normalizamos\n",
        "frame = processor.process_state_batch(obs_preprocessed)\n",
        "# Visualizar en escala de grises\n",
        "plt.imshow(frame, cmap='gray')\n",
        "plt.show()\n",
        "print(observation.shape)\n",
        "print(obs_preprocessed.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "vHJaGcAVelRY",
        "outputId": "f2686c2b-a159-4c1f-e857-984352520683"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 160, 3)\n",
            "(84, 84)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clase FrameStack para apilar frames"
      ],
      "metadata": {
        "id": "xVe0dhbsfvc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FrameStack:\n",
        "    \"\"\"\n",
        "    Clase para gestionar una pila de fotogramas consecutivos del entorno, utilizada para capturar\n",
        "    el contexto temporal en juegos de Atari (e.g., SpaceInvaders-v0).\n",
        "\n",
        "    Mantiene una cola (deque) de fotogramas preprocesados con un tamaño máximo definido por\n",
        "    max_length, apilándolos para formar un estado con información de movimiento.\n",
        "\n",
        "    Atributos:\n",
        "    ----------\n",
        "        frames (deque): Cola de fotogramas preprocesados con longitud máxima max_length.\n",
        "        max_length (int): Número máximo de fotogramas a apilar (e.g., WINDOW_LENGTH).\n",
        "\n",
        "    MÉTODOS:\n",
        "    --------\n",
        "        append(frame): Añade un nuevo fotograma a la pila, eliminando el más antiguo si es necesario.\n",
        "        get_stacked_state(): Devuelve el estado apilado como un array NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_frames=4):\n",
        "        \"\"\"\n",
        "        Inicializa la pila de fotogramas.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "            max_length (int): Número máximo de fotogramas a mantener en la pila.\n",
        "        \"\"\"\n",
        "        self.num_frames = num_frames\n",
        "        self.frames = deque([np.zeros((INPUT_SHAPE), dtype=np.int) for i in range(self.num_frames)], maxlen=self.num_frames)\n",
        "\n",
        "    def reset(self):\n",
        "        self.frames.clear()\n",
        "\n",
        "    def add_frame(self, frame, is_new_episode):\n",
        "        \"\"\"\n",
        "        Añade un fotograma preprocesado a la pila.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "            frame (np.ndarray): Fotograma preprocesado (e.g., imagen en escala de grises de 84x84).\n",
        "        \"\"\"\n",
        "        # Si es el primer frame, llenamos el deque\n",
        "        if is_new_episode:\n",
        "            self.frames = deque([np.zeros((INPUT_SHAPE), dtype=np.int) for i in range(self.num_frames)], maxlen=self.num_frames)\n",
        "            for _ in range(self.num_frames):\n",
        "                self.frames.append(frame)\n",
        "        else:\n",
        "            self.frames.append(frame)\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"\n",
        "        Devuelve el estado apilado como un array NumPy con los fotogramas actuales.\n",
        "\n",
        "        Si la pila no está llena, repite el último fotograma hasta completar max_length.\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "            np.ndarray: Array de forma (84, 84, max_length) con los fotogramas apilados.\n",
        "        \"\"\"\n",
        "        # Convertir a array con shape (84, 84, 4)\n",
        "        return np.stack(self.frames, axis=-1)"
      ],
      "metadata": {
        "id": "gSJsJa41fzo7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clase ReplayMemory"
      ],
      "metadata": {
        "id": "j-yCJoGjf2Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"ReplayMemory optimizada para evitar fugas de memoria\"\"\"\n",
        "\n",
        "    def __init__(self, capacity, state_shape):\n",
        "        self.capacity = capacity\n",
        "        self.position = 0\n",
        "        self.size = 0\n",
        "\n",
        "        # Pre-asignar arrays con el tamaño exacto\n",
        "        # Usar uint8 para estados (más eficiente que float32)\n",
        "        self.states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
        "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
        "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
        "        self.next_states = np.zeros((capacity, *state_shape), dtype=np.uint8)\n",
        "        self.dones = np.zeros(capacity, dtype=np.bool_)\n",
        "\n",
        "        print(f\"ReplayMemory creada: {capacity} samples, {state_shape} shape\")\n",
        "        memory_size = (\n",
        "            self.states.nbytes + self.next_states.nbytes +\n",
        "            self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n",
        "        ) / (1024 * 1024)\n",
        "        print(f\"Memoria asignada: {memory_size:.2f} MB\")\n",
        "\n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Añade una experiencia al buffer de forma eficiente\"\"\"\n",
        "        # Convertir a uint8 para ahorrar memoria (estados son imágenes 0-255)\n",
        "        if state.dtype != np.uint8:\n",
        "            state = (state * 255).astype(np.uint8)\n",
        "        if next_state.dtype != np.uint8:\n",
        "            next_state = (next_state * 255).astype(np.uint8)\n",
        "\n",
        "        # Almacenar directamente en el array pre-asignado\n",
        "        self.states[self.position] = state\n",
        "        self.actions[self.position] = action\n",
        "        self.rewards[self.position] = reward\n",
        "        self.next_states[self.position] = next_state\n",
        "        self.dones[self.position] = done\n",
        "\n",
        "        # Actualizar posición circular\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Muestra un batch de experiencias de forma eficiente\"\"\"\n",
        "        if self.size < batch_size:\n",
        "            raise ValueError(f\"No hay suficientes samples ({self.size}) para batch_size ({batch_size})\")\n",
        "\n",
        "        # Generar índices aleatorios\n",
        "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
        "\n",
        "        # Extraer batch y convertir de vuelta a float32 para el entrenamiento\n",
        "        batch_states = self.states[indices].astype(np.float32) / 255.0\n",
        "        batch_actions = self.actions[indices]\n",
        "        batch_rewards = self.rewards[indices]\n",
        "        batch_next_states = self.next_states[indices].astype(np.float32) / 255.0\n",
        "        batch_dones = self.dones[indices]\n",
        "\n",
        "        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Limpia la memoria de forma segura\"\"\"\n",
        "        self.position = 0\n",
        "        self.size = 0\n",
        "        # No es necesario limpiar los arrays, se sobrescriben"
      ],
      "metadata": {
        "id": "ewKKozUaf-mG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clase PerformanceMonitor"
      ],
      "metadata": {
        "id": "No-SaTPRkQoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase para monitoreo de memoria y rendimiento\n",
        "class PerformanceMonitor(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, save_path='diagnosticos'):\n",
        "        self.save_path = save_path\n",
        "        self.episode_times = []\n",
        "        self.memory_usage = []\n",
        "        self.current_episode = 0\n",
        "        self.episode_start_time = None\n",
        "        self.episode_start_memory = None\n",
        "\n",
        "    def on_episode_begin(self, episode, logs={}):\n",
        "        self.episode_start_time = time.time()\n",
        "        self.episode_start_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "        self.current_episode = episode\n",
        "        print(f\"Episodio {episode} comenzando. Memoria inicial: {self.episode_start_memory:.2f} MB\")\n",
        "\n",
        "    def on_episode_end(self, episode, logs={}):\n",
        "        end_time = time.time()\n",
        "        final_memory = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "        episode_duration = end_time - self.episode_start_time\n",
        "\n",
        "        self.episode_times.append(episode_duration)\n",
        "        self.memory_usage.append(final_memory)\n",
        "\n",
        "        print(f\"Episodio {episode} completado en {episode_duration:.2f} segundos\")\n",
        "        print(f\"Memoria final: {final_memory:.2f} MB (cambio: {final_memory - self.episode_start_memory:.2f} MB)\")\n",
        "\n",
        "        # Guardar diagnóstico cada 5 episodios\n",
        "        if (episode + 1) % 5 == 0:\n",
        "            self.save_diagnostics(episode)\n",
        "\n",
        "        # Forzar recolección de basura\n",
        "        gc.collect()\n",
        "\n",
        "    def save_diagnostics(self, episode):\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.episode_times)\n",
        "        plt.title('Tiempo por episodio')\n",
        "        plt.ylabel('Segundos')\n",
        "        plt.xlabel('Episodio')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.memory_usage)\n",
        "        plt.title('Uso de memoria')\n",
        "        plt.ylabel('MB')\n",
        "        plt.xlabel('Episodio')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.save_path}/rendimiento_episodio_{episode+1}.png\")\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "Wf2A5kDokNdS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Implementación de la red neuronal\n",
        "\n",
        "#### Definición de las redes neuronales"
      ],
      "metadata": {
        "id": "iTgDOJoCgISN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crearemos una clase para construir un red Q-profunda, con tres capas convolucionales, seguidas de una capa de aplanamiento y una capa completamente conectada."
      ],
      "metadata": {
        "id": "SFAzP0UigPVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CQ0dGSAUgP0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "class DQNetwork(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Red neuronal Deep Q-Network (DQN) para aproximar la función Q en aprendizaje por refuerzo.\n",
        "\n",
        "    Esta clase implementa una red convolucional que recibe un estado (conjunto de frames)\n",
        "    y produce los valores Q para cada acción posible. Usa capas convolucionales seguidas\n",
        "    de capas totalmente conectadas, con activación ELU.\n",
        "\n",
        "    Parámetros:\n",
        "    -----------\n",
        "    state_size : tupla/lista - Dimensiones del estado de entrada (por ejemplo, [84, 84, 4]).\n",
        "    action_size : int        - Número de acciones posibles en el entorno.\n",
        "    learning_rate : float    - Tasa de aprendizaje para el optimizador Adam.\n",
        "    name : str, opcional     - Nombre del scope de TensorFlow para distinguir múltiples redes.\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "        super(DQNetwork, self).__init__(name=name)\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        with tf.name_scope(name):\n",
        "            # Definir las capas\n",
        "            if K.image_data_format() == 'channels_last':\n",
        "                self.permute = Permute((2, 3, 1))\n",
        "            elif K.image_data_format() == 'channels_first':\n",
        "                self.permute = Permute((1, 2, 3))\n",
        "            else:\n",
        "                raise RuntimeError('Unknown image_dim_ordering.')\n",
        "\n",
        "            self.conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu')\n",
        "            self.conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')\n",
        "            self.conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n",
        "            self.flatten = Flatten()\n",
        "            self.fc = Dense(512, activation='relu')\n",
        "            self.output_layer = Dense(action_size, activation='linear')\n",
        "\n",
        "            # Define loss function\n",
        "            # Definir el optimizador\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "            self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Ejecuta la red neuronal para un batch de estados y define la lógica para hacer la propagación hacia adelante\n",
        "        (forward pass).  Se llama automáticamente en una clase que hereda de tf.keras.Model o tf.keras.layers.Layer.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "        inputs : tf.Tensor  - Tensor con los estados de entrada, shape = (batch_size, *state_size)\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "        tf.Tensor - Valores Q para cada acción, shape = (batch_size, action_size)\n",
        "        \"\"\"\n",
        "        x = self.permute(inputs)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            'state_size': self.state_size,\n",
        "            'action_size': self.action_size,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'name': self.name,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ],
      "metadata": {
        "id": "lSrKAOx8Ok6Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de DDQNetwork independiente\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class DDQNetwork(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Implementación de una red Double DQN para juegos de Atari.\n",
        "\n",
        "    Esta arquitectura utiliza dos redes para reducir la sobreestimación de valores Q:\n",
        "    - La red principal (main_network) para seleccionar acciones\n",
        "    - La red objetivo (target_network) para calcular valores Q objetivo\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size=(84, 84, 4), action_size=6, learning_rate=0.00025, name='DDQNetwork'):\n",
        "        super(DDQNetwork, self).__init__(name=name)\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Definir arquitectura de redes (main y target son idénticas en estructura)\n",
        "\n",
        "        # Red principal - para selección de acciones\n",
        "        self.main_network = self._build_network()\n",
        "\n",
        "        # Red objetivo - para evaluación de valores Q\n",
        "        self.target_network = self._build_network()\n",
        "\n",
        "        # Inicialmente, ambas redes tienen los mismos pesos\n",
        "        self.update_target(tau=1.0)\n",
        "\n",
        "        # Optimizador\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        # Función de pérdida\n",
        "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    def _build_network(self):\n",
        "        \"\"\"Construye la arquitectura de la red neuronal.\"\"\"\n",
        "        inputs = Input(shape=self.state_size)\n",
        "\n",
        "        # Capas convolucionales\n",
        "        x = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', padding='valid')(inputs)\n",
        "        x = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', padding='valid')(x)\n",
        "        x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='valid')(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        # Capas densas\n",
        "        x = Dense(512, activation='relu')(x)\n",
        "        outputs = Dense(self.action_size, activation='linear')(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward pass a través de la red principal.\n",
        "        \"\"\"\n",
        "        return self.main_network(inputs, training=training)\n",
        "\n",
        "    def update_target(self, tau=1.0):\n",
        "        \"\"\"\n",
        "        Actualiza los pesos de la red objetivo.\n",
        "\n",
        "        Args:\n",
        "            tau: Factor de mezcla para actualización suave (1.0 = copia completa)\n",
        "        \"\"\"\n",
        "        # Obtener pesos actuales\n",
        "        main_weights = self.main_network.get_weights()\n",
        "        target_weights = self.target_network.get_weights()\n",
        "\n",
        "        # Actualización suave de los pesos\n",
        "        updated_weights = []\n",
        "        for i in range(len(target_weights)):\n",
        "            updated_weights.append((1 - tau) * target_weights[i] + tau * main_weights[i])\n",
        "\n",
        "        # Establecer nuevos pesos\n",
        "        self.target_network.set_weights(updated_weights)\n",
        "\n",
        "    def train_step_optimized(self, states, actions, target_q):\n",
        "        \"\"\"\n",
        "        Realiza un paso de entrenamiento: calcula la pérdida y aplica gradientes.\n",
        "\n",
        "        Parámetros:\n",
        "        -----------\n",
        "        states : tf.Tensor -    Batch de estados de entrada, shape = (batch_size, *state_size)\n",
        "        actions : tf.Tensor -   Acciones tomadas, codificadas one-hot, shape = (batch_size, action_size)\n",
        "        target_q : tf.Tensor -  Valores objetivo Q, shape = (batch_size,)\n",
        "\n",
        "        Retorna:\n",
        "        --------\n",
        "        loss : tf.Tensor -      Valor de la pérdida calculada en este paso.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Calculamos los valores Q para todos los estados y acciones usando la red principal\n",
        "            q_values = self.main_network(states, training=True)\n",
        "\n",
        "            # Obtenemos los valores Q para las acciones tomadas\n",
        "            q_action = tf.reduce_sum(q_values * actions, axis=1)\n",
        "\n",
        "            # target_q ya debe venir calculado con la red objetivo\n",
        "            # Si target_q ya es (batch_size,), usamos directamente:\n",
        "            target_q_action = target_q\n",
        "\n",
        "            # Calculamos la pérdida MSE entre Q predicho y target_Q\n",
        "            loss = self.loss_fn(target_q_action, q_action)\n",
        "\n",
        "        # Calculamos los gradientes y actualizamos solo la red principal\n",
        "        gradients = tape.gradient(loss, self.main_network.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.main_network.trainable_variables))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Método para serialización.\"\"\"\n",
        "        return {\n",
        "            'state_size': self.state_size,\n",
        "            'action_size': self.action_size,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'name': self.name\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        \"\"\"Método para deserialización.\"\"\"\n",
        "        return cls(**config)"
      ],
      "metadata": {
        "id": "qm4vt4leyndU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Implementación de la solución DQN\n",
        "\n",
        "#### Funciones auxiliares"
      ],
      "metadata": {
        "id": "d0DHvKNshvQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapear clases a prefijos\n",
        "class_to_prefix = {\n",
        "        DQNetwork: \"DQN\",\n",
        "        DQNetwork: \"DDQN\" #,\n",
        "    #    DDQNetworkWithReplay: \"DDQN_Replay\",\n",
        "    #    DuelingDQNetworkWithReplay: \"DuelingDQN_Replay\"\n",
        "}"
      ],
      "metadata": {
        "id": "3pyO_DovhzyW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create a fixed version of the compute_q_values function\n",
        "def fixed_compute_q_values(self, state):\n",
        "    \"\"\"Compute Q-values for a given state.\"\"\"\n",
        "    # Get batch Q-values\n",
        "    q_values = self.compute_batch_q_values([state])\n",
        "\n",
        "    # Convert TensorFlow tensor to NumPy if needed\n",
        "    if isinstance(q_values, tf.Tensor):\n",
        "        q_values = q_values.numpy()\n",
        "\n",
        "    # Debug output\n",
        "    print(f\"Q-values shape: {q_values.shape}, nb_actions: {self.nb_actions}\")\n",
        "\n",
        "    # Flatten the Q-values - handle different shapes\n",
        "    if len(q_values.shape) > 1:\n",
        "        if q_values.shape[1] == self.nb_actions:\n",
        "            q_values = tf.keras.backend.get_value( q_values[0])\n",
        "        else:\n",
        "            # If shape doesn't match expected, reshape to match nb_actions\n",
        "            q_values = q_values.reshape(-1)[:self.nb_actions]\n",
        "\n",
        "    # Make sure we have the right number of Q-values\n",
        "    if len(q_values) != self.nb_actions:\n",
        "        print(f\"WARNING: Q-values shape {q_values.shape} doesn't match nb_actions {self.nb_actions}\")\n",
        "        # Pad or truncate to match nb_actions\n",
        "        if len(q_values) < self.nb_actions:\n",
        "            # Pad with zeros\n",
        "            q_values = np.pad(q_values, (0, self.nb_actions - len(q_values)))\n",
        "        else:\n",
        "            # Truncate\n",
        "            q_values = q_values[:self.nb_actions]\n",
        "\n",
        "    return q_values\n",
        "\n",
        "# Step 3: Create a fixed version of compute_batch_q_values\n",
        "def fixed_compute_batch_q_values(self, state_batch):\n",
        "    \"\"\"Compute Q-values for a batch of states.\"\"\"\n",
        "    # Process state batch\n",
        "    batch = self.process_state_batch(state_batch)\n",
        "\n",
        "    # Print shape for debugging\n",
        "    print(f\"Processed batch shape: {batch.shape}\")\n",
        "\n",
        "    try:\n",
        "        # Convert to tensor with correct dtype\n",
        "        tensor_batch = tf.convert_to_tensor(batch, dtype=tf.float32)\n",
        "\n",
        "        # Get Q-values using the model's call method directly\n",
        "        q_values = self.model(tensor_batch, training=False)\n",
        "\n",
        "        # Convert to NumPy if it's a tensor\n",
        "        if isinstance(q_values, tf.Tensor):\n",
        "            q_values = tf.keras.backend.get_value( q_values)\n",
        "\n",
        "        return q_values\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing batch Q-values: {e}\")\n",
        "        # Return default Q-values as fallback\n",
        "        return np.zeros((len(state_batch), self.nb_actions))\n"
      ],
      "metadata": {
        "id": "ocXuoi6rmQaQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Apply our fixes to the DQN agent\n",
        "DQNAgent.compute_q_values = fixed_compute_q_values\n",
        "DQNAgent.compute_batch_q_values = fixed_compute_batch_q_values"
      ],
      "metadata": {
        "id": "z2Afsxq6mfb8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ENTRENAMIENTO** ***********"
      ],
      "metadata": {
        "id": "zgbzJyUjmTzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar tracemalloc\n",
        "tracemalloc.start()\n",
        "# Suprimir advertencias\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Configurar TensorFlow\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "#state_size = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE  # e.g. (4, 84, 84)\n",
        "\n",
        "def crear_agente(env, dqnet_class, processor, state_size, action_size, gamma, prefijo,\n",
        "                     learning_rate=0.00025, checkpoint_path='checkpoints',\n",
        "                     input_shape=(84, 84, 4), memoria_tamano=1000000,\n",
        "                     warmup_steps=50000, target_update_interval=10000, target_update_tau=0.001,\n",
        "                     epsilon_start=1.0, epsilon_min=0.1, epsilon_steps=1000000):\n",
        "    \"\"\"\n",
        "    Configura un agente DQN o DDQN con parámetros comunes.\n",
        "\n",
        "    Parámetros:\n",
        "    -----------\n",
        "    dqnet_class: Clase del modelo (e.g., DQNetwork, DDQNetwork, DDQNetworkWithReplay, DuelingDQNetworkWithReplay).\n",
        "            Se usa para verificar si el modelo soporta memoria de repetición y para determinar el prefijo de los\n",
        "            nombres de archivo de checkpoint.\n",
        "    env : gym.Env -        Entorno de OpenAI Gym\n",
        "    input_shape : tuple -  Forma del estado de entrada (altura, ancho, canales)\n",
        "    memoria_tamano : int - Tamaño de la memoria de experiencia\n",
        "    warmup_steps : int -   Pasos antes de empezar a aprender\n",
        "    target_update_interval : int - Frecuencia de actualización de la red objetivo\n",
        "    target_update_tau : float - Factor de actualización suave para la red objetivo\n",
        "    learning_rate : float -     Tasa de aprendizaje para el optimizador\n",
        "    epsilon_start, epsilon_min, epsilon_steps : float, float, int - Parámetros para la política epsilon-greedy con decaimiento lineal\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    agent : DQNAgent -       Agente configurado según los parámetros\n",
        "    model : tf.keras.Model o DDQNetwork - Modelo usado por el agente\n",
        "    processor : AtariProcessor -          Procesador de observaciones\n",
        "    callbacks : list -                    Lista de callbacks configurados\n",
        "    \"\"\"\n",
        "    # Limpiar cualquier grafo anterior\n",
        "    gc.collect()\n",
        "\n",
        "    # Verificar que el entorno esté disponible\n",
        "    if env is None:\n",
        "        raise ValueError(\"Debe proporcionar un entorno válido\")\n",
        "\n",
        "    # Paso condicional de memory_size solo para DDQNetworkWithReplay\n",
        "    # if dqnet_class in [DDQNetworkWithReplay, DuelingDQNetworkWithReplay]:\n",
        "    #     model_base = dqnet_class(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau)\n",
        "    # elif dqnet_class in [DDQNetwork]:  # si esta clase sí necesita tau\n",
        "    #     model_base = dqnet_class(state_size, action_size, learning_rate, tau=tau, gamma=gamma)\n",
        "    # else:\n",
        "    model_base = dqnet_class(state_size, action_size, learning_rate)\n",
        "    enable_double_dqn = False\n",
        "\n",
        "    # Wrap funcional para Keras-RL\n",
        "    dummy_input = tf.keras.Input(shape=input_shape)\n",
        "    output = model_base(dummy_input)\n",
        "    model = tf.keras.Model(inputs=dummy_input, outputs=output)\n",
        "    model.summary()\n",
        "\n",
        "    memory = SequentialMemory(limit=memoria_tamano, window_length=input_shape[2])\n",
        "\n",
        "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                              value_max=epsilon_start, value_min=epsilon_min, value_test=.05,\n",
        "                              nb_steps=epsilon_steps)\n",
        "    # Crear agente\n",
        "    dqn = DQNAgent(model=model,\n",
        "                  nb_actions=nb_actions,\n",
        "                  policy=policy,\n",
        "                  memory=memory,\n",
        "                  processor=processor,\n",
        "                  nb_steps_warmup=warmup_steps,\n",
        "                  # Configuración según tipo\n",
        "                  target_model_update=target_update_interval if dqnet_class  in [DQNetwork] else 1,\n",
        "                  enable_double_dqn=enable_double_dqn,\n",
        "                  gamma=gamma,\n",
        "                  train_interval=4)\n",
        "\n",
        "    # Compilar el agente\n",
        "    dqn.compile(Adam(learning_rate=learning_rate), metrics=['mae'])\n",
        "\n",
        "    # Configurar callbacks\n",
        "    tipo_str = prefijo\n",
        "    weights_filename = f'{tipo_str}_weights.h5f'\n",
        "    checkpoint_weights_filename = f'{tipo_str}_weights_{{step}}.h5f'\n",
        "    log_filename = f'{tipo_str}_log.json'\n",
        "    callbacks = [\n",
        "        ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000),\n",
        "        FileLogger(log_filename, interval=100),\n",
        "        PerformanceMonitor(save_path='diagnosticos')\n",
        "    ]\n",
        "\n",
        "    # Para DDQN, añadimos un callback simple para actualizar la red target\n",
        "    if dqnet_class  in [DQNetwork]:\n",
        "        class SimpleTargetUpdate(tf.keras.callbacks.Callback):\n",
        "            def __init__(self, ddqn_model, update_interval, tau):\n",
        "                self.ddqn_model = ddqn_model\n",
        "                self.update_interval = update_interval\n",
        "                self.tau = tau\n",
        "                self.step_counter = 0\n",
        "\n",
        "            def on_step_end(self, step, logs={}):\n",
        "                self.step_counter += 1\n",
        "                if self.step_counter % self.update_interval == 0:\n",
        "                    self.ddqn_model.update_target(tau=self.tau)\n",
        "                    print(f\"Paso {self.step_counter}: Actualizada red objetivo con tau={self.tau}\")\n",
        "\n",
        "        callbacks.append(SimpleTargetUpdate(model, target_update_interval, target_update_tau))\n",
        "\n",
        "    return dqn, model, callbacks"
      ],
      "metadata": {
        "id": "6Lopk_9tiB0I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kM65PKIxQic2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de utilidad para crear y comparar los modelos que se vayan creando\n",
        "def entrenar_modelo(model_type,\n",
        "                 state_size,\n",
        "                 action_size,\n",
        "                 total_episodios,\n",
        "                 max_steps,\n",
        "                 batch_size,\n",
        "                 gamma,\n",
        "                 epsilon_start,\n",
        "                 memory_size,\n",
        "                 tau,\n",
        "                 learning_rate=0.001,\n",
        "                 start_episode=0,\n",
        "                 checkpoint_path=checkpoint_path,\n",
        "                 memory_path=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Factory function para crear modelos DQN o DDQN.\n",
        "\n",
        "    Parámetros:\n",
        "    -----------\n",
        "        model_type : str -       Tipo de modelo a entrenar. Puede ser 'DQN' o 'DDQN'.\n",
        "        state_size : tuple -     Dimensiones del estado de entrada (por ejemplo, (84, 84, 4)).\n",
        "        action_size : int-       Número de acciones posibles en el entorno.\n",
        "        total_episodios : int-   Número total de episodios de entrenamiento.\n",
        "        max_steps : int -        Número máximo de pasos por episodio.\n",
        "        batch_size : int -       Tamaño de los lotes para el entrenamiento.\n",
        "        gamma : float -          Factor de descuento para los futuros Q-valores.\n",
        "        epsilon_start : float -  Valor inicial de epsilon (probabilidad de exploración).\n",
        "        memory_size : int -      Tamaño máximo de la memoria de repetición (replay buffer).\n",
        "        tau : float -            Tasa de actualización suave para redes objetivo (target network).\n",
        "        learning_rate : float -  Tasa de aprendizaje para el optimizador (default: 0.001).\n",
        "        start_episode : int   -  Episodio desde el cual comenzar (por ejemplo, al reanudar desde un checkpoint) (default: 0).\n",
        "        checkpoint_path : str -  Ruta al archivo de checkpoint (pesos del modelo guardados en formato `.h5`) para continuar entrenamiento (default: None).\n",
        "        memory_path : str -      Ruta al archivo de memoria de repetición guardada (`.pkl`) para restaurar la experiencia pasada (default: None).\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "        DQNetwork o DDQNetwork - Modelo creado según el tipo especificado\n",
        "    \"\"\"\n",
        "    # Crear el procesador Atari\n",
        "    processor = AtariProcessor()\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    if model_type.upper() == 'DQN':\n",
        "        print(\"Entrenando DQN simple para Space Invaders...\")\n",
        "        # Entrenar -------------------------------------\n",
        "        trained_dqn, dqn_model, dqn_callbacks = crear_agente(env, dqnet_class, processor, state_size,\n",
        "                     action_size, gamma, prefijo=model_type.upper(),\n",
        "                     learning_rate=0.00025, checkpoint_path='checkpoints',\n",
        "                     input_shape=(84, 84, 4), memoria_tamano=1000000,\n",
        "                     warmup_steps=50000, target_update_interval=10000, target_update_tau=0.001,\n",
        "                     epsilon_start=1.0, epsilon_min=0.1, epsilon_steps=1000000)\n",
        "    elif model_type.upper() == 'DDQN':\n",
        "        print(\"Entrenando DDQN simple para Space Invaders...\")\n",
        "        trained_dqn, dqn_model, dqn_callbacks = crear_agente(env, dqnet_class, processor, state_size,\n",
        "                     action_size, gamma, prefijo=model_type.upper(),\n",
        "                     learning_rate=0.00025, checkpoint_path='checkpoints',\n",
        "                     input_shape=(84, 84, 4), memoria_tamano=1000000,\n",
        "                     warmup_steps=50000, target_update_interval=10000, target_update_tau=0.001,\n",
        "                     epsilon_start=1.0, epsilon_min=0.1, epsilon_steps=1000000)\n",
        "    elif model_type.upper() == 'DDQN_REPLAY':\n",
        "        print(\"Entrenando DDQN con Replay Memory para Space Invaders...\")\n",
        "        trained_dqn, dqn_model, dqn_callbacks = crear_agente(env, dqnet_class, processor, state_size,\n",
        "                     action_size, gamma, prefijo=model_type.upper(),\n",
        "                     learning_rate=0.00025, checkpoint_path='checkpoints',\n",
        "                     input_shape=(84, 84, 4), memoria_tamano=1000000,\n",
        "                     warmup_steps=50000, target_update_interval=10000, target_update_tau=0.001,\n",
        "                     epsilon_start=1.0, epsilon_min=0.1, epsilon_steps=1000000)\n",
        "    elif model_type.upper() == 'DUELING_DQN_REPLAY':\n",
        "        print(\"Entrenando Dueling DQN con Replay Memory para Space Invaders...\")\n",
        "        trained_dqn, dqn_model, dqn_callbacks = crear_agente(env, dqnet_class, processor, state_size,\n",
        "                     action_size, gamma, prefijo=model_type.upper(),\n",
        "                     learning_rate=0.00025, checkpoint_path='checkpoints',\n",
        "                     input_shape=(84, 84, 4), memoria_tamano=1000000,\n",
        "                     warmup_steps=50000, target_update_interval=10000, target_update_tau=0.001,\n",
        "                     epsilon_start=1.0, epsilon_min=0.1, epsilon_steps=1000000)\n",
        "    else:\n",
        "        raise ValueError(\"model_type debe ser 'DQN', 'DDQN', 'DDQN_REPLAY' o 'DUELING_DQN_REPLAY' \")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Entrenar el agente DQN\n",
        "    trained_dqn.fit(env, nb_steps=1000000, callbacks=dqn_callbacks, verbose=2)\n",
        "\n",
        "    return trained_dqn"
      ],
      "metadata": {
        "id": "MTCkURI3oARu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Bloque de Ejecución Principal ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Crear un grafo explícito\n",
        "        with tf.Graph().as_default() as graph:\n",
        "            # Control global de si se entrena o solo se carga\n",
        "            training_global = True\n",
        "            # Control de renderizado durante el entrenamiento (no afecta la grabación de video final)\n",
        "            episode_render = False\n",
        "\n",
        "            # ------------ ENTRENAMIENTO Y CARGA DE MEJORES MODELOS --------------------------------------\n",
        "\n",
        "            # Diccionario para guardar los *mejores modelos cargados/entrenados* de cada tipo\n",
        "            trained_models = {}\n",
        "            # Lista de tuplas (nombre_modelo, clase_modelo, flag_entrenamiento_especifico)\n",
        "            modelos_a_procesar = [\n",
        "                ('DQN', DQNetwork, True), # NO Entrenar DQN\n",
        "                ('DDQN', DDQNetwork, False), # No entrenar DDQN, solo cargar si existe\n",
        "                ('DDQN_REPLAY', DQNetwork, False), # No entrenar DDQN_REPLAY\n",
        "                ('DUELING_DQN_REPLAY', DQNetwork, False) # Entrenar DuelingDQN_REPLAY\n",
        "            ]\n",
        "\n",
        "            for model_name, model_class, training_specific_flag in modelos_a_procesar:\n",
        "                tf.keras.backend.clear_session()  # Limpiar antes de cada modelo\n",
        "                # La bandera de entrenamiento final es la global AND la específica del modelo\n",
        "                entrenarSN = training_global and training_specific_flag\n",
        "                model_instance = None\n",
        "                if entrenarSN:\n",
        "                    # Llama a la función crear_modelo existente para el entrenamiento.\n",
        "                    # crear_modelo ya maneja los parámetros específicos de cada tipo de red.\n",
        "                    model_instance = entrenar_modelo(\n",
        "                        model_name, state_size, action_size,\n",
        "                        total_episodios, max_steps, batch_size, gamma, epsilon_start, # Pasando los nuevos parámetros\n",
        "                        memory_size=memory_size, tau=tau, learning_rate=learning_rate,\n",
        "                        start_episode=0,checkpoint_path=checkpoint_path\n",
        "                    )\n",
        "                else:\n",
        "                    # Si no estamos entrenando, creamos una instancia vacía para cargar pesos.\n",
        "                    # Necesitamos instanciar la clase de modelo directamente.\n",
        "                    if model_name in ['DDQN_REPLAY', 'DUELING_DQN_REPLAY']:\n",
        "                        model_instance = model_class(state_size, action_size, learning_rate, memory_size=memory_size, tau=tau)\n",
        "                    elif model_name == 'DDQN':\n",
        "                        model_instance = model_class(state_size, action_size, learning_rate, tau=tau)\n",
        "                    else: # 'DQN'\n",
        "                        model_instance = model_class(state_size, action_size, learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Entrenar el agente\n",
        "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=100, visualize=False)\n",
        "\n",
        "    # Guardar pesos finales\n",
        "    dqn.save_weights(weights_filename, overwrite=True)\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CAsEf6rOl0GF",
        "outputId": "b14a2866-c6ff-40c7-c74f-0cf468a57f78"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 3, 84, 84)]       0         \n",
            "                                                                 \n",
            " DQNetwork (DQNetwork)       (None, 6)                 1685158   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,685,158\n",
            "Trainable params: 1,685,158\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 1750000 steps ...\n",
            "Episodio 0 comenzando. Memoria inicial: 775.79 MB\n",
            "Interval 1 (0 steps performed)\n",
            "Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  1/100 [..............................] - ETA: 22s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  2/100 [..............................] - ETA: 11s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  3/100 [..............................] - ETA: 11s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  4/100 [>.............................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  5/100 [>.............................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  6/100 [>.............................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  7/100 [=>............................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  8/100 [=>............................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  9/100 [=>............................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 10/100 [==>...........................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 11/100 [==>...........................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 12/100 [==>...........................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 13/100 [==>...........................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 14/100 [===>..........................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 15/100 [===>..........................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 16/100 [===>..........................] - ETA: 10s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 17/100 [====>.........................] - ETA: 9s - reward: 0.0000e+00 Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 18/100 [====>.........................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 19/100 [====>.........................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 20/100 [=====>........................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 21/100 [=====>........................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 22/100 [=====>........................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 23/100 [=====>........................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 24/100 [======>.......................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 25/100 [======>.......................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 26/100 [======>.......................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 27/100 [=======>......................] - ETA: 9s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 28/100 [=======>......................] - ETA: 8s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 29/100 [=======>......................] - ETA: 8s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 30/100 [========>.....................] - ETA: 8s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 31/100 [========>.....................] - ETA: 8s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 32/100 [========>.....................] - ETA: 8s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 33/100 [========>.....................] - ETA: 8s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 34/100 [=========>....................] - ETA: 8s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 35/100 [=========>....................] - ETA: 8s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 36/100 [=========>....................] - ETA: 8s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 37/100 [==========>...................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 38/100 [==========>...................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 39/100 [==========>...................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 40/100 [===========>..................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 41/100 [===========>..................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 42/100 [===========>..................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 43/100 [===========>..................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 44/100 [============>.................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 45/100 [============>.................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 46/100 [============>.................] - ETA: 7s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 47/100 [=============>................] - ETA: 6s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 48/100 [=============>................] - ETA: 6s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 49/100 [=============>................] - ETA: 6s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 50/100 [==============>...............] - ETA: 6s - reward: 0.0200    Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 51/100 [==============>...............] - ETA: 6s - reward: 0.0196Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 52/100 [==============>...............] - ETA: 6s - reward: 0.0192Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 53/100 [==============>...............] - ETA: 6s - reward: 0.0189Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 54/100 [===============>..............] - ETA: 6s - reward: 0.0185Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 55/100 [===============>..............] - ETA: 6s - reward: 0.0182Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 56/100 [===============>..............] - ETA: 5s - reward: 0.0179Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 57/100 [================>.............] - ETA: 5s - reward: 0.0175Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 58/100 [================>.............] - ETA: 5s - reward: 0.0172Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 59/100 [================>.............] - ETA: 5s - reward: 0.0169Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 60/100 [=================>............] - ETA: 5s - reward: 0.0167Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 61/100 [=================>............] - ETA: 5s - reward: 0.0164Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 62/100 [=================>............] - ETA: 5s - reward: 0.0161Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 63/100 [=================>............] - ETA: 5s - reward: 0.0159Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 64/100 [==================>...........] - ETA: 5s - reward: 0.0156Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 65/100 [==================>...........] - ETA: 4s - reward: 0.0154Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 66/100 [==================>...........] - ETA: 4s - reward: 0.0152Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 67/100 [===================>..........] - ETA: 4s - reward: 0.0149Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 68/100 [===================>..........] - ETA: 4s - reward: 0.0147Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 69/100 [===================>..........] - ETA: 4s - reward: 0.0145Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 70/100 [====================>.........] - ETA: 4s - reward: 0.0143Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 71/100 [====================>.........] - ETA: 4s - reward: 0.0141Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 72/100 [====================>.........] - ETA: 4s - reward: 0.0139Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 73/100 [====================>.........] - ETA: 4s - reward: 0.0137Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 74/100 [=====================>........] - ETA: 4s - reward: 0.0135Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 75/100 [=====================>........] - ETA: 3s - reward: 0.0133Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 76/100 [=====================>........] - ETA: 3s - reward: 0.0132Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 77/100 [======================>.......] - ETA: 3s - reward: 0.0130Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 78/100 [======================>.......] - ETA: 3s - reward: 0.0128Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 79/100 [======================>.......] - ETA: 3s - reward: 0.0127Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 80/100 [=======================>......] - ETA: 3s - reward: 0.0125Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 81/100 [=======================>......] - ETA: 2s - reward: 0.0123Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 82/100 [=======================>......] - ETA: 2s - reward: 0.0122Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 83/100 [=======================>......] - ETA: 2s - reward: 0.0120Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 84/100 [========================>.....] - ETA: 2s - reward: 0.0119Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 85/100 [========================>.....] - ETA: 2s - reward: 0.0118Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 86/100 [========================>.....] - ETA: 2s - reward: 0.0116Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 87/100 [=========================>....] - ETA: 2s - reward: 0.0115Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 88/100 [=========================>....] - ETA: 1s - reward: 0.0114Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 89/100 [=========================>....] - ETA: 1s - reward: 0.0112Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 90/100 [==========================>...] - ETA: 1s - reward: 0.0111Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 91/100 [==========================>...] - ETA: 1s - reward: 0.0110Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 92/100 [==========================>...] - ETA: 1s - reward: 0.0109Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 93/100 [==========================>...] - ETA: 1s - reward: 0.0108Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 94/100 [===========================>..] - ETA: 0s - reward: 0.0106Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 95/100 [===========================>..] - ETA: 0s - reward: 0.0105Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 96/100 [===========================>..] - ETA: 0s - reward: 0.0104Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 97/100 [============================>.] - ETA: 0s - reward: 0.0103Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 98/100 [============================>.] - ETA: 0s - reward: 0.0102Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 99/100 [============================>.] - ETA: 0s - reward: 0.0101Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "100/100 [==============================] - 16s 162ms/step - reward: 0.0100\n",
            "Interval 2 (100 steps performed)\n",
            "Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  1/100 [..............................] - ETA: 19s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  2/100 [..............................] - ETA: 18s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  3/100 [..............................] - ETA: 19s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  4/100 [>.............................] - ETA: 18s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  5/100 [>.............................] - ETA: 18s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  6/100 [>.............................] - ETA: 18s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  7/100 [=>............................] - ETA: 18s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  8/100 [=>............................] - ETA: 18s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            "  9/100 [=>............................] - ETA: 18s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 10/100 [==>...........................] - ETA: 18s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 11/100 [==>...........................] - ETA: 18s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 12/100 [==>...........................] - ETA: 17s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 13/100 [==>...........................] - ETA: 17s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 14/100 [===>..........................] - ETA: 17s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 15/100 [===>..........................] - ETA: 17s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 16/100 [===>..........................] - ETA: 17s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 17/100 [====>.........................] - ETA: 16s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 18/100 [====>.........................] - ETA: 16s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 19/100 [====>.........................] - ETA: 16s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 20/100 [=====>........................] - ETA: 16s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 21/100 [=====>........................] - ETA: 16s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 22/100 [=====>........................] - ETA: 16s - reward: 0.0000e+00Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 23/100 [=====>........................] - ETA: 15s - reward: 0.0435    Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 24/100 [======>.......................] - ETA: 15s - reward: 0.0417Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 25/100 [======>.......................] - ETA: 16s - reward: 0.0400Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 26/100 [======>.......................] - ETA: 16s - reward: 0.0385Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 27/100 [=======>......................] - ETA: 16s - reward: 0.0370Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 28/100 [=======>......................] - ETA: 16s - reward: 0.0357Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 29/100 [=======>......................] - ETA: 16s - reward: 0.0345Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 30/100 [========>.....................] - ETA: 16s - reward: 0.0333Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 31/100 [========>.....................] - ETA: 16s - reward: 0.0323Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 32/100 [========>.....................] - ETA: 16s - reward: 0.0312Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 33/100 [========>.....................] - ETA: 15s - reward: 0.0303Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 34/100 [=========>....................] - ETA: 15s - reward: 0.0294Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 35/100 [=========>....................] - ETA: 15s - reward: 0.0286Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 36/100 [=========>....................] - ETA: 15s - reward: 0.0278Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 37/100 [==========>...................] - ETA: 14s - reward: 0.0270Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 38/100 [==========>...................] - ETA: 14s - reward: 0.0263Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 39/100 [==========>...................] - ETA: 14s - reward: 0.0256Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 40/100 [===========>..................] - ETA: 14s - reward: 0.0250Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 41/100 [===========>..................] - ETA: 13s - reward: 0.0244Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 42/100 [===========>..................] - ETA: 13s - reward: 0.0238Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 43/100 [===========>..................] - ETA: 13s - reward: 0.0233Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 44/100 [============>.................] - ETA: 13s - reward: 0.0227Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 45/100 [============>.................] - ETA: 13s - reward: 0.0222Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 46/100 [============>.................] - ETA: 12s - reward: 0.0217Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 47/100 [=============>................] - ETA: 12s - reward: 0.0213Processed batch shape: (1, 3, 84, 84)\n",
            "Q-values shape: (1, 6), nb_actions: 6\n",
            " 48/100 [=============>................] - ETA: 12s - reward: 0.0208Processed batch shape: (1, 3, 84, 84)\n",
            "done, took 28.147 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "Graph execution error:\n\nDetected at node 'Read_11/ReadVariableOp' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipython-input-29-1746447107.py\", line 1, in <cell line: 0>\n      simple_train()\n    File \"/tmp/ipython-input-28-594255280.py\", line 58, in simple_train\n      dqn.save_weights(weights_filename, overwrite=True)\n    File \"/usr/local/lib/python3.11/dist-packages/rl/agents/dqn.py\", line 209, in save_weights\n      self.model.save_weights(filepath, overwrite=overwrite)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 2898, in save_weights\n      saving_api.save_weights(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/saving/saving_api.py\", line 230, in save_weights\n      legacy_sm_saving_lib.save_weights(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/saving/legacy/save.py\", line 371, in save_weights\n      model._checkpoint.write(filepath, options=options)\nNode: 'Read_11/ReadVariableOp'\nCould not find variable DQNetwork_6/learning_rate. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/DQNetwork_6/learning_rate/N10tensorflow3VarE does not exist.\n\t [[{{node Read_11/ReadVariableOp}}]]\n\nOriginal stack trace for 'Read_11/ReadVariableOp':\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipython-input-29-1746447107.py\", line 1, in <cell line: 0>\n    simple_train()\n  File \"/tmp/ipython-input-28-594255280.py\", line 58, in simple_train\n    dqn.save_weights(weights_filename, overwrite=True)\n  File \"/usr/local/lib/python3.11/dist-packages/rl/agents/dqn.py\", line 209, in save_weights\n    self.model.save_weights(filepath, overwrite=overwrite)\n  File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 2898, in save_weights\n    saving_api.save_weights(\n  File \"/usr/local/lib/python3.11/dist-packages/keras/saving/saving_api.py\", line 230, in save_weights\n    legacy_sm_saving_lib.save_weights(\n  File \"/usr/local/lib/python3.11/dist-packages/keras/saving/legacy/save.py\", line 371, in save_weights\n    model._checkpoint.write(filepath, options=options)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\", line 2256, in write\n    return self._write(file_prefix, options)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\", line 2291, in _write\n    output = self._saver.save(file_prefix=file_prefix, options=options)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\", line 1304, in save\n    save_path, new_feed_additions = self._save_cached_when_graph_building(\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\", line 1242, in _save_cached_when_graph_building\n    save_op = saver.save(file_prefix, options=options)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/functional_saver.py\", line 407, in save\n    return save_fn()\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/functional_saver.py\", line 381, in save_fn\n    sharded_saves.append(saver.save(shard_prefix, options))\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/functional_saver.py\", line 68, in save\n    tensor_value = tensor.tensor\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/training/saving/saveable_object.py\", line 53, in tensor\n    return self._tensor() if callable(self._tensor) else self._tensor\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 699, in _read_variable_closure\n    x = v.read_value_no_copy()\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 790, in read_value_no_copy\n    value = self._read_variable_op(no_copy=True)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 753, in _read_variable_op\n    result = read_and_set_handle(no_copy)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 743, in read_and_set_handle\n    result = gen_resource_variable_ops.read_variable_op(\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 538, in read_variable_op\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 3814, in _create_op_internal\n    ret = Operation(\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-29-1746447107.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimple_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-28-594255280.py\u001b[0m in \u001b[0;36msimple_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Guardar pesos finales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1395\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1397\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node 'Read_11/ReadVariableOp' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipython-input-29-1746447107.py\", line 1, in <cell line: 0>\n      simple_train()\n    File \"/tmp/ipython-input-28-594255280.py\", line 58, in simple_train\n      dqn.save_weights(weights_filename, overwrite=True)\n    File \"/usr/local/lib/python3.11/dist-packages/rl/agents/dqn.py\", line 209, in save_weights\n      self.model.save_weights(filepath, overwrite=overwrite)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 2898, in save_weights\n      saving_api.save_weights(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/saving/saving_api.py\", line 230, in save_weights\n      legacy_sm_saving_lib.save_weights(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/saving/legacy/save.py\", line 371, in save_weights\n      model._checkpoint.write(filepath, options=options)\nNode: 'Read_11/ReadVariableOp'\nCould not find variable DQNetwork_6/learning_rate. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Resource localhost/DQNetwork_6/learning_rate/N10tensorflow3VarE does not exist.\n\t [[{{node Read_11/ReadVariableOp}}]]\n\nOriginal stack trace for 'Read_11/ReadVariableOp':\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipython-input-29-1746447107.py\", line 1, in <cell line: 0>\n    simple_train()\n  File \"/tmp/ipython-input-28-594255280.py\", line 58, in simple_train\n    dqn.save_weights(weights_filename, overwrite=True)\n  File \"/usr/local/lib/python3.11/dist-packages/rl/agents/dqn.py\", line 209, in save_weights\n    self.model.save_weights(filepath, overwrite=overwrite)\n  File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 2898, in save_weights\n    saving_api.save_weights(\n  File \"/usr/local/lib/python3.11/dist-packages/keras/saving/saving_api.py\", line 230, in save_weights\n    legacy_sm_saving_lib.save_weights(\n  File \"/usr/local/lib/python3.11/dist-packages/keras/saving/legacy/save.py\", line 371, in save_weights\n    model._checkpoint.write(filepath, options=options)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\", line 2256, in write\n    return self._write(file_prefix, options)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\", line 2291, in _write\n    output = self._saver.save(file_prefix=file_prefix, options=options)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\", line 1304, in save\n    save_path, new_feed_additions = self._save_cached_when_graph_building(\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/checkpoint.py\", line 1242, in _save_cached_when_graph_building\n    save_op = saver.save(file_prefix, options=options)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/functional_saver.py\", line 407, in save\n    return save_fn()\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/functional_saver.py\", line 381, in save_fn\n    sharded_saves.append(saver.save(shard_prefix, options))\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/checkpoint/functional_saver.py\", line 68, in save\n    tensor_value = tensor.tensor\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/training/saving/saveable_object.py\", line 53, in tensor\n    return self._tensor() if callable(self._tensor) else self._tensor\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 699, in _read_variable_closure\n    x = v.read_value_no_copy()\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 790, in read_value_no_copy\n    value = self._read_variable_op(no_copy=True)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 753, in _read_variable_op\n    result = read_and_set_handle(no_copy)\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 743, in read_and_set_handle\n    result = gen_resource_variable_ops.read_variable_op(\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 538, in read_variable_op\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\", line 3814, in _create_op_internal\n    ret = Operation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY4KZPwP43MQ"
      },
      "outputs": [],
      "source": [
        "# Testing part\n",
        "#weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "weights_filename = \"dqn_BreakoutDeterministic-v4_weights_1750000.h5f\"\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68dzFiUS2U6L"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}